{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ABR Data Analysis and Preprocessing Validation\n",
        "\n",
        "This notebook analyzes the ABR Excel data to understand its structure, identify potential issues in the preprocessing pipeline, and suggest improvements.\n",
        "\n",
        "## Objectives:\n",
        "1. **Data Structure Analysis**: Understand the Excel file structure and columns\n",
        "2. **Data Quality Assessment**: Check for missing values, outliers, and inconsistencies\n",
        "3. **Preprocessing Validation**: Validate the current preprocessing logic\n",
        "4. **Issue Identification**: Find potential problems in the preprocessing pipeline\n",
        "5. **Improvement Suggestions**: Propose enhancements for better data handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "Pandas version: 1.5.3\n",
            "NumPy version: 1.26.4\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Load and Explore Data Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from: data/abr_data_preprocessed.xlsx\n",
            "âœ… Data loaded successfully!\n",
            "Dataset shape: (55237, 496)\n",
            "Memory usage: 249.89 MB\n"
          ]
        }
      ],
      "source": [
        "# Load the Excel data\n",
        "excel_path = \"data/abr_data_preprocessed.xlsx\"\n",
        "print(f\"Loading data from: {excel_path}\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_excel(excel_path)\n",
        "    print(f\"âœ… Data loaded successfully!\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading data: {e}\")\n",
        "    df = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š COLUMN ANALYSIS\n",
            "==================================================\n",
            "Total columns: 496\n",
            "Column names (first 20): ['ID', 'Name', 'Patient_ID', 'Stimulus Side', 'Age', 'Birth Date', 'Frequency', 'Gender', 'Intensity', 'Protocol', 'Stimulus Polarity', 'Stimulus Rate', 'Stimulus Type', 'Sweeps Measured', 'Sweeps Rejected', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'I Latancy']\n",
            "Column names (last 20): ['452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', 'FMP', 'High pass hardware', 'Low Pass hardware', 'ResNo']\n",
            "\n",
            "ğŸ“ˆ Numeric columns: 483\n",
            "ğŸ“ Object columns: 13\n",
            "Object columns: ['Name', 'Stimulus Side', 'Birth Date', 'Gender', 'Protocol', 'Stimulus Polarity', 'Stimulus Type', 'Test Date', 'Hear Loss - Left', 'Hear Loss - Right', 'Hear_Loss', 'High pass hardware', 'Low Pass hardware']\n",
            "\n",
            "â° Time series columns: 467\n",
            "Time series range: 1 to 99\n",
            "\n",
            "ğŸ“‹ DATA TYPES:\n",
            "float64    477\n",
            "object      13\n",
            "int64        6\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore column structure\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š COLUMN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total columns: {len(df.columns)}\")\n",
        "    print(f\"Column names (first 20): {list(df.columns[:20])}\")\n",
        "    print(f\"Column names (last 20): {list(df.columns[-20:])}\")\n",
        "    \n",
        "    # Identify different types of columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"ğŸ“ Object columns: {len(object_cols)}\")\n",
        "    print(f\"Object columns: {object_cols}\")\n",
        "    \n",
        "    # Check for time series columns (numbered columns)\n",
        "    time_series_cols = [col for col in df.columns if col.isdigit()]\n",
        "    print(f\"\\nâ° Time series columns: {len(time_series_cols)}\")\n",
        "    if time_series_cols:\n",
        "        print(f\"Time series range: {min(time_series_cols)} to {max(time_series_cols)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nğŸ“‹ DATA TYPES:\")\n",
        "    print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Analyze Key Columns for Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” KEY COLUMNS ANALYSIS\n",
            "==================================================\n",
            "ğŸ“‹ Critical columns availability:\n",
            "  âœ… FMP: Available\n",
            "  âœ… Stimulus Polarity: Available\n",
            "  âœ… Age: Available\n",
            "  âœ… Intensity: Available\n",
            "  âœ… Stimulus Rate: Available\n",
            "  âœ… Hear_Loss: Available\n",
            "\n",
            "ğŸ“‹ Latency columns availability:\n",
            "  âœ… I Latancy: Available\n",
            "  âœ… III Latancy: Available\n",
            "  âœ… V Latancy: Available\n",
            "\n",
            "ğŸ“‹ Amplitude columns availability:\n",
            "  âœ… I Amplitude: Available\n",
            "  âœ… III Amplitude: Available\n",
            "  âœ… V Amplitude: Available\n"
          ]
        }
      ],
      "source": [
        "# Analyze key columns used in preprocessing\n",
        "if df is not None:\n",
        "    print(\"ğŸ” KEY COLUMNS ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Check for critical columns used in preprocessing\n",
        "    critical_columns = ['FMP', 'Stimulus Polarity', 'Age', 'Intensity', 'Stimulus Rate', 'Hear_Loss']\n",
        "    latency_columns = ['I Latancy', 'III Latancy', 'V Latancy']\n",
        "    amplitude_columns = ['I Amplitude', 'III Amplitude', 'V Amplitude']\n",
        "    \n",
        "    print(\"ğŸ“‹ Critical columns availability:\")\n",
        "    for col in critical_columns:\n",
        "        if col in df.columns:\n",
        "            print(f\"  âœ… {col}: Available\")\n",
        "        else:\n",
        "            print(f\"  âŒ {col}: Missing\")\n",
        "            # Try to find similar column names\n",
        "            similar = [c for c in df.columns if col.lower() in c.lower() or c.lower() in col.lower()]\n",
        "            if similar:\n",
        "                print(f\"     Similar columns found: {similar}\")\n",
        "    \n",
        "    print(\"\\nğŸ“‹ Latency columns availability:\")\n",
        "    for col in latency_columns:\n",
        "        if col in df.columns:\n",
        "            print(f\"  âœ… {col}: Available\")\n",
        "        else:\n",
        "            print(f\"  âŒ {col}: Missing\")\n",
        "            similar = [c for c in df.columns if 'latency' in c.lower() or 'latancy' in c.lower()]\n",
        "            if similar:\n",
        "                print(f\"     Similar columns found: {similar}\")\n",
        "    \n",
        "    print(\"\\nğŸ“‹ Amplitude columns availability:\")\n",
        "    for col in amplitude_columns:\n",
        "        if col in df.columns:\n",
        "            print(f\"  âœ… {col}: Available\")\n",
        "        else:\n",
        "            print(f\"  âŒ {col}: Missing\")\n",
        "            similar = [c for c in df.columns if 'amplitude' in c.lower()]\n",
        "            if similar:\n",
        "                print(f\"     Similar columns found: {similar}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ” DATA QUALITY ANALYSIS\n",
            "==================================================\n",
            "ğŸ“Š Overall missing values: 556,593 / 27,397,552 (2.03%)\n",
            "\n",
            "ğŸ“‹ Missing values in critical columns:\n",
            "  FMP: 20 (0.04%)\n",
            "  Stimulus Polarity: 0 (0.00%)\n",
            "  Age: 1,930 (3.49%)\n",
            "  Intensity: 0 (0.00%)\n",
            "  Stimulus Rate: 0 (0.00%)\n",
            "  Hear_Loss: 0 (0.00%)\n",
            "\n",
            "ğŸ“‹ Missing values in latency columns:\n",
            "  I Latancy: 49,196 (89.06%)\n",
            "  III Latancy: 48,671 (88.11%)\n",
            "  V Latancy: 15,075 (27.29%)\n",
            "\n",
            "ğŸ“‹ Missing values in amplitude columns:\n",
            "  I Amplitude: 49,196 (89.06%)\n",
            "  III Amplitude: 48,671 (88.11%)\n",
            "  V Amplitude: 15,075 (27.29%)\n",
            "\n",
            "â° Time series missing values: 254,445 / 25,795,679 (0.99%)\n"
          ]
        }
      ],
      "source": [
        "# Analyze data quality and missing values\n",
        "if df is not None:\n",
        "    print(\"\\nğŸ” DATA QUALITY ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Overall missing values\n",
        "    total_missing = df.isnull().sum().sum()\n",
        "    total_cells = df.shape[0] * df.shape[1]\n",
        "    missing_percentage = (total_missing / total_cells) * 100\n",
        "    \n",
        "    print(f\"ğŸ“Š Overall missing values: {total_missing:,} / {total_cells:,} ({missing_percentage:.2f}%)\")\n",
        "    \n",
        "    # Missing values by column type\n",
        "    print(f\"\\nğŸ“‹ Missing values in critical columns:\")\n",
        "    for col in critical_columns:\n",
        "        if col in df.columns:\n",
        "            missing_count = df[col].isnull().sum()\n",
        "            missing_pct = (missing_count / len(df)) * 100\n",
        "            print(f\"  {col}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
        "    \n",
        "    print(f\"\\nğŸ“‹ Missing values in latency columns:\")\n",
        "    for col in latency_columns:\n",
        "        if col in df.columns:\n",
        "            missing_count = df[col].isnull().sum()\n",
        "            missing_pct = (missing_count / len(df)) * 100\n",
        "            print(f\"  {col}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
        "    \n",
        "    print(f\"\\nğŸ“‹ Missing values in amplitude columns:\")\n",
        "    for col in amplitude_columns:\n",
        "        if col in df.columns:\n",
        "            missing_count = df[col].isnull().sum()\n",
        "            missing_pct = (missing_count / len(df)) * 100\n",
        "            print(f\"  {col}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
        "    \n",
        "    # Check time series columns for missing values\n",
        "    if time_series_cols:\n",
        "        ts_missing = df[time_series_cols].isnull().sum().sum()\n",
        "        ts_total = len(df) * len(time_series_cols)\n",
        "        ts_missing_pct = (ts_missing / ts_total) * 100\n",
        "        print(f\"\\nâ° Time series missing values: {ts_missing:,} / {ts_total:,} ({ts_missing_pct:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Validate Preprocessing Filters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ PREPROCESSING FILTER VALIDATION\n",
            "==================================================\n",
            "ğŸ“Š Initial dataset size: 55,237 records\n",
            "\n",
            "ğŸ“ˆ FMP Statistics:\n",
            "  Min: 0.00\n",
            "  Max: 1515.17\n",
            "  Mean: 2.39\n",
            "  Median: 0.74\n",
            "  Records with FMP > 2.0: 15,443 (28.0%)\n",
            "  Records removed by FMP filter: 39,794 (72.0%)\n",
            "\n",
            "ğŸ“Š Stimulus Polarity distribution:\n",
            "  Alternate: 53,280 (96.5%)\n",
            "  Rarefaction: 980 (1.8%)\n",
            "  Condensation: 977 (1.8%)\n",
            "  Records with 'Alternate' polarity: 53,280\n",
            "\n",
            "ğŸ”„ Combined filter effect:\n",
            "  Records after both filters: 15,162 (27.4%)\n",
            "  Total records removed: 40,075 (72.6%)\n"
          ]
        }
      ],
      "source": [
        "# Test the preprocessing filters\n",
        "if df is not None:\n",
        "    print(\"ğŸ”§ PREPROCESSING FILTER VALIDATION\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    initial_count = len(df)\n",
        "    print(f\"ğŸ“Š Initial dataset size: {initial_count:,} records\")\n",
        "    \n",
        "    # Filter 1: FMP > 2.0\n",
        "    if 'FMP' in df.columns:\n",
        "        fmp_stats = df['FMP'].describe()\n",
        "        print(f\"\\nğŸ“ˆ FMP Statistics:\")\n",
        "        print(f\"  Min: {fmp_stats['min']:.2f}\")\n",
        "        print(f\"  Max: {fmp_stats['max']:.2f}\")\n",
        "        print(f\"  Mean: {fmp_stats['mean']:.2f}\")\n",
        "        print(f\"  Median: {fmp_stats['50%']:.2f}\")\n",
        "        \n",
        "        fmp_filtered = df[df['FMP'] > 2.0]\n",
        "        fmp_count = len(fmp_filtered)\n",
        "        fmp_removed = initial_count - fmp_count\n",
        "        print(f\"  Records with FMP > 2.0: {fmp_count:,} ({(fmp_count/initial_count)*100:.1f}%)\")\n",
        "        print(f\"  Records removed by FMP filter: {fmp_removed:,} ({(fmp_removed/initial_count)*100:.1f}%)\")\n",
        "    else:\n",
        "        print(\"âŒ FMP column not found!\")\n",
        "    \n",
        "    # Filter 2: Stimulus Polarity\n",
        "    if 'Stimulus Polarity' in df.columns:\n",
        "        polarity_counts = df['Stimulus Polarity'].value_counts()\n",
        "        print(f\"\\nğŸ“Š Stimulus Polarity distribution:\")\n",
        "        for polarity, count in polarity_counts.items():\n",
        "            percentage = (count / len(df)) * 100\n",
        "            print(f\"  {polarity}: {count:,} ({percentage:.1f}%)\")\n",
        "        \n",
        "        if 'Alternate' in polarity_counts:\n",
        "            alternate_count = polarity_counts['Alternate']\n",
        "            print(f\"  Records with 'Alternate' polarity: {alternate_count:,}\")\n",
        "        else:\n",
        "            print(\"  âš ï¸ No 'Alternate' polarity found!\")\n",
        "    else:\n",
        "        print(\"âŒ Stimulus Polarity column not found!\")\n",
        "    \n",
        "    # Combined filter effect\n",
        "    if 'FMP' in df.columns and 'Stimulus Polarity' in df.columns:\n",
        "        combined_filtered = df[(df['FMP'] > 2.0) & (df['Stimulus Polarity'] == 'Alternate')]\n",
        "        combined_count = len(combined_filtered)\n",
        "        combined_removed = initial_count - combined_count\n",
        "        print(f\"\\nğŸ”„ Combined filter effect:\")\n",
        "        print(f\"  Records after both filters: {combined_count:,} ({(combined_count/initial_count)*100:.1f}%)\")\n",
        "        print(f\"  Total records removed: {combined_removed:,} ({(combined_removed/initial_count)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Analyze Static Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š STATIC PARAMETERS ANALYSIS\n",
            "==================================================\n",
            "\n",
            "ğŸ“ˆ Age Analysis:\n",
            "  Count: 53307\n",
            "  Mean: 2.88\n",
            "  Std: 8.05\n",
            "  Min: 0.00\n",
            "  25%: 0.00\n",
            "  50%: 0.00\n",
            "  75%: 2.00\n",
            "  Max: 75.00\n",
            "  Outliers (IQR method): 4812 (8.71%)\n",
            "  Missing values: 1,930 (3.49%)\n",
            "\n",
            "ğŸ“ˆ Intensity Analysis:\n",
            "  Count: 55237\n",
            "  Mean: 46.02\n",
            "  Std: 30.84\n",
            "  Min: 0.00\n",
            "  25%: 20.00\n",
            "  50%: 40.00\n",
            "  75%: 75.00\n",
            "  Max: 100.00\n",
            "  Outliers (IQR method): 0 (0.00%)\n",
            "  Missing values: 0 (0.00%)\n",
            "\n",
            "ğŸ“ˆ Stimulus Rate Analysis:\n",
            "  Count: 55237\n",
            "  Mean: 33.66\n",
            "  Std: 3.67\n",
            "  Min: 11.10\n",
            "  25%: 33.10\n",
            "  50%: 33.10\n",
            "  75%: 33.10\n",
            "  Max: 49.10\n",
            "  Outliers (IQR method): 10461 (18.94%)\n",
            "  Missing values: 0 (0.00%)\n",
            "\n",
            "ğŸ“ˆ Hear_Loss Analysis:\n",
            "  Unique values: 5\n",
            "  Value distribution:\n",
            "    NORMAL: 43,346 (78.5%)\n",
            "    SNÄ°K: 6,012 (10.9%)\n",
            "    Ä°TÄ°K: 3,054 (5.5%)\n",
            "    TOTAL: 2,266 (4.1%)\n",
            "    NÃ–ROPATÄ°: 559 (1.0%)\n",
            "  Missing values: 0 (0.00%)\n"
          ]
        }
      ],
      "source": [
        "# Analyze static parameters in detail\n",
        "if df is not None:\n",
        "    print(\"ğŸ“Š STATIC PARAMETERS ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    static_params = ['Age', 'Intensity', 'Stimulus Rate', 'Hear_Loss']\n",
        "    \n",
        "    for param in static_params:\n",
        "        if param in df.columns:\n",
        "            print(f\"\\nğŸ“ˆ {param} Analysis:\")\n",
        "            \n",
        "            if df[param].dtype in ['object', 'category']:\n",
        "                # Categorical analysis\n",
        "                value_counts = df[param].value_counts()\n",
        "                print(f\"  Unique values: {df[param].nunique()}\")\n",
        "                print(f\"  Value distribution:\")\n",
        "                for value, count in value_counts.head(10).items():\n",
        "                    percentage = (count / len(df)) * 100\n",
        "                    print(f\"    {value}: {count:,} ({percentage:.1f}%)\")\n",
        "                if len(value_counts) > 10:\n",
        "                    print(f\"    ... and {len(value_counts) - 10} more values\")\n",
        "            else:\n",
        "                # Numerical analysis\n",
        "                stats = df[param].describe()\n",
        "                print(f\"  Count: {stats['count']:.0f}\")\n",
        "                print(f\"  Mean: {stats['mean']:.2f}\")\n",
        "                print(f\"  Std: {stats['std']:.2f}\")\n",
        "                print(f\"  Min: {stats['min']:.2f}\")\n",
        "                print(f\"  25%: {stats['25%']:.2f}\")\n",
        "                print(f\"  50%: {stats['50%']:.2f}\")\n",
        "                print(f\"  75%: {stats['75%']:.2f}\")\n",
        "                print(f\"  Max: {stats['max']:.2f}\")\n",
        "                \n",
        "                # Check for outliers\n",
        "                Q1 = stats['25%']\n",
        "                Q3 = stats['75%']\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                outliers = df[(df[param] < lower_bound) | (df[param] > upper_bound)][param]\n",
        "                print(f\"  Outliers (IQR method): {len(outliers)} ({(len(outliers)/len(df))*100:.2f}%)\")\n",
        "            \n",
        "            # Missing values\n",
        "            missing = df[param].isnull().sum()\n",
        "            print(f\"  Missing values: {missing:,} ({(missing/len(df))*100:.2f}%)\")\n",
        "        else:\n",
        "            print(f\"\\nâŒ {param}: Column not found!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Analyze Clinical Features (Latency & Amplitude)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”¬ CLINICAL FEATURES ANALYSIS\n",
            "==================================================\n",
            "\n",
            "â±ï¸ LATENCY ANALYSIS:\n",
            "\n",
            "ğŸ“Š I Latancy:\n",
            "  Valid samples: 6,041 / 55,237 (10.9%)\n",
            "  Mean: 1.575\n",
            "  Std: 0.480\n",
            "  Min: -1.000\n",
            "  Max: 20.930\n",
            "  Values outside typical range (0.5, 3.0): 96 (1.59%)\n",
            "\n",
            "ğŸ“Š III Latancy:\n",
            "  Valid samples: 6,566 / 55,237 (11.9%)\n",
            "  Mean: 4.137\n",
            "  Std: 0.491\n",
            "  Min: 1.670\n",
            "  Max: 8.600\n",
            "  Values outside typical range (0.5, 3.0): 6557 (99.86%)\n",
            "\n",
            "ğŸ“Š V Latancy:\n",
            "  Valid samples: 40,162 / 55,237 (72.7%)\n",
            "  Mean: 7.820\n",
            "  Std: 1.708\n",
            "  Min: 4.800\n",
            "  Max: 18.470\n",
            "  Values outside typical range (4.0, 8.0): 15653 (38.97%)\n",
            "\n",
            "ğŸ“ˆ AMPLITUDE ANALYSIS:\n",
            "\n",
            "ğŸ“Š I Amplitude:\n",
            "  Valid samples: 6,041 / 55,237 (10.9%)\n",
            "  Mean: 0.302\n",
            "  Std: 0.175\n",
            "  Min: -1.332\n",
            "  Max: 1.786\n",
            "  Negative values: 142 (2.35%)\n",
            "  Values > 2.0: 0 (0.00%)\n",
            "\n",
            "ğŸ“Š III Amplitude:\n",
            "  Valid samples: 6,566 / 55,237 (11.9%)\n",
            "  Mean: 0.398\n",
            "  Std: 0.224\n",
            "  Min: -1.320\n",
            "  Max: 1.783\n",
            "  Negative values: 117 (1.78%)\n",
            "  Values > 2.0: 0 (0.00%)\n",
            "\n",
            "ğŸ“Š V Amplitude:\n",
            "  Valid samples: 40,162 / 55,237 (72.7%)\n",
            "  Mean: 0.174\n",
            "  Std: 0.162\n",
            "  Min: -1.255\n",
            "  Max: 1.496\n",
            "  Negative values: 3316 (8.26%)\n",
            "  Values > 2.0: 0 (0.00%)\n"
          ]
        }
      ],
      "source": [
        "# Analyze latency and amplitude data\n",
        "if df is not None:\n",
        "    print(\"ğŸ”¬ CLINICAL FEATURES ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Latency analysis\n",
        "    print(\"\\nâ±ï¸ LATENCY ANALYSIS:\")\n",
        "    for col in latency_columns:\n",
        "        if col in df.columns:\n",
        "            print(f\"\\nğŸ“Š {col}:\")\n",
        "            \n",
        "            # Basic statistics\n",
        "            valid_data = df[col].dropna()\n",
        "            if len(valid_data) > 0:\n",
        "                stats = valid_data.describe()\n",
        "                print(f\"  Valid samples: {len(valid_data):,} / {len(df):,} ({(len(valid_data)/len(df))*100:.1f}%)\")\n",
        "                print(f\"  Mean: {stats['mean']:.3f}\")\n",
        "                print(f\"  Std: {stats['std']:.3f}\")\n",
        "                print(f\"  Min: {stats['min']:.3f}\")\n",
        "                print(f\"  Max: {stats['max']:.3f}\")\n",
        "                \n",
        "                # Check for physiologically reasonable values\n",
        "                # Typical ABR latencies: Wave I ~1.5ms, Wave III ~3.5-4ms, Wave V ~5.5-6.5ms\n",
        "                if 'I' in col:\n",
        "                    reasonable_range = (0.5, 3.0)\n",
        "                elif 'III' in col:\n",
        "                    reasonable_range = (2.0, 6.0)\n",
        "                elif 'V' in col:\n",
        "                    reasonable_range = (4.0, 8.0)\n",
        "                else:\n",
        "                    reasonable_range = (0, 10)\n",
        "                \n",
        "                out_of_range = valid_data[(valid_data < reasonable_range[0]) | (valid_data > reasonable_range[1])]\n",
        "                print(f\"  Values outside typical range {reasonable_range}: {len(out_of_range)} ({(len(out_of_range)/len(valid_data))*100:.2f}%)\")\n",
        "            else:\n",
        "                print(f\"  âŒ No valid data found!\")\n",
        "        else:\n",
        "            print(f\"\\nâŒ {col}: Column not found!\")\n",
        "    \n",
        "    # Amplitude analysis\n",
        "    print(\"\\nğŸ“ˆ AMPLITUDE ANALYSIS:\")\n",
        "    for col in amplitude_columns:\n",
        "        if col in df.columns:\n",
        "            print(f\"\\nğŸ“Š {col}:\")\n",
        "            \n",
        "            # Basic statistics\n",
        "            valid_data = df[col].dropna()\n",
        "            if len(valid_data) > 0:\n",
        "                stats = valid_data.describe()\n",
        "                print(f\"  Valid samples: {len(valid_data):,} / {len(df):,} ({(len(valid_data)/len(df))*100:.1f}%)\")\n",
        "                print(f\"  Mean: {stats['mean']:.3f}\")\n",
        "                print(f\"  Std: {stats['std']:.3f}\")\n",
        "                print(f\"  Min: {stats['min']:.3f}\")\n",
        "                print(f\"  Max: {stats['max']:.3f}\")\n",
        "                \n",
        "                # Check for negative values (which might be problematic)\n",
        "                negative_count = (valid_data < 0).sum()\n",
        "                print(f\"  Negative values: {negative_count} ({(negative_count/len(valid_data))*100:.2f}%)\")\n",
        "                \n",
        "                # Check for very large values (potential outliers)\n",
        "                very_large = valid_data[valid_data > 2.0]  # Amplitudes > 2Î¼V are quite large\n",
        "                print(f\"  Values > 2.0: {len(very_large)} ({(len(very_large)/len(valid_data))*100:.2f}%)\")\n",
        "            else:\n",
        "                print(f\"  âŒ No valid data found!\")\n",
        "        else:\n",
        "            print(f\"\\nâŒ {col}: Column not found!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Analyze Time Series Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â° TIME SERIES ANALYSIS\n",
            "==================================================\n",
            "ğŸ“Š Time series overview:\n",
            "  Total time points: 467\n",
            "  Range: 1 to 467\n",
            "  First 200 points available: True\n",
            "  Available columns for first 200 points: 200\n",
            "\n",
            "ğŸ“ˆ Time series statistics:\n",
            "  Shape: (55237, 200)\n",
            "  Data type: float64\n",
            "  Memory usage: 84.28 MB\n",
            "  Missing values: 0 / 11,047,400 (0.00%)\n",
            "  Min value: -8.122000\n",
            "  Max value: 11.743000\n",
            "  Mean value: 0.007722\n",
            "  Std value: 0.163739\n",
            "  Rows with constant values: 2 (0.00%)\n",
            "  Extreme values (>3Ïƒ): 128452 (1.16%)\n"
          ]
        }
      ],
      "source": [
        "# Analyze time series data\n",
        "if df is not None and time_series_cols:\n",
        "    print(\"â° TIME SERIES ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Convert time series columns to numeric (they should be strings like '1', '2', etc.)\n",
        "    time_series_numeric = [int(col) for col in time_series_cols if col.isdigit()]\n",
        "    time_series_numeric.sort()\n",
        "    \n",
        "    print(f\"ğŸ“Š Time series overview:\")\n",
        "    print(f\"  Total time points: {len(time_series_numeric)}\")\n",
        "    print(f\"  Range: {min(time_series_numeric)} to {max(time_series_numeric)}\")\n",
        "    print(f\"  First 200 points available: {max(time_series_numeric) >= 200}\")\n",
        "    \n",
        "    # Analyze first 200 time points (as used in preprocessing)\n",
        "    first_200_cols = [str(i) for i in range(1, 201) if str(i) in df.columns]\n",
        "    print(f\"  Available columns for first 200 points: {len(first_200_cols)}\")\n",
        "    \n",
        "    if first_200_cols:\n",
        "        # Get time series data for analysis\n",
        "        ts_data = df[first_200_cols].values\n",
        "        \n",
        "        print(f\"\\nğŸ“ˆ Time series statistics:\")\n",
        "        print(f\"  Shape: {ts_data.shape}\")\n",
        "        print(f\"  Data type: {ts_data.dtype}\")\n",
        "        print(f\"  Memory usage: {ts_data.nbytes / 1024**2:.2f} MB\")\n",
        "        \n",
        "        # Check for missing values\n",
        "        ts_missing = np.isnan(ts_data).sum()\n",
        "        ts_total = ts_data.size\n",
        "        print(f\"  Missing values: {ts_missing:,} / {ts_total:,} ({(ts_missing/ts_total)*100:.2f}%)\")\n",
        "        \n",
        "        # Basic statistics\n",
        "        valid_data = ts_data[~np.isnan(ts_data)]\n",
        "        if len(valid_data) > 0:\n",
        "            print(f\"  Min value: {valid_data.min():.6f}\")\n",
        "            print(f\"  Max value: {valid_data.max():.6f}\")\n",
        "            print(f\"  Mean value: {valid_data.mean():.6f}\")\n",
        "            print(f\"  Std value: {valid_data.std():.6f}\")\n",
        "            \n",
        "            # Check for constant values (which might indicate issues)\n",
        "            constant_rows = []\n",
        "            for i in range(ts_data.shape[0]):\n",
        "                row = ts_data[i, :]\n",
        "                valid_row = row[~np.isnan(row)]\n",
        "                if len(valid_row) > 1 and np.std(valid_row) < 1e-10:\n",
        "                    constant_rows.append(i)\n",
        "            \n",
        "            print(f\"  Rows with constant values: {len(constant_rows)} ({(len(constant_rows)/ts_data.shape[0])*100:.2f}%)\")\n",
        "            \n",
        "            # Check for extreme values\n",
        "            extreme_threshold = 3 * np.std(valid_data)\n",
        "            extreme_values = np.abs(valid_data) > extreme_threshold\n",
        "            print(f\"  Extreme values (>3Ïƒ): {extreme_values.sum()} ({(extreme_values.sum()/len(valid_data))*100:.2f}%)\")\n",
        "    else:\n",
        "        print(\"âŒ No time series columns found for first 200 points!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Test Current Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ PREPROCESSING PIPELINE TEST\n",
            "==================================================\n",
            "ğŸ“Š Testing with subset: 1000 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-09 04:42:32,106 - INFO - Loading Excel data from temp_test_data.xlsx\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ”„ Running preprocessing steps...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-09 04:42:35,907 - INFO - Loaded 1000 records with 496 columns\n",
            "2025-06-09 04:42:35,908 - INFO - Applying filtering criteria...\n",
            "2025-06-09 04:42:35,910 - INFO - After FMP > 2.0 filter: 243 records (757 removed)\n",
            "2025-06-09 04:42:35,911 - INFO - After alternate polarity filter: 235 records (8 removed)\n",
            "2025-06-09 04:42:35,913 - INFO - After removing missing critical data: 182 records (53 removed)\n",
            "2025-06-09 04:42:35,913 - INFO - Total filtering removed 818 records (81.8%)\n",
            "2025-06-09 04:42:35,914 - INFO - Extracting time series data (first 200 timestamps)\n",
            "2025-06-09 04:42:35,915 - INFO - Extracted time series shape: (182, 200)\n",
            "2025-06-09 04:42:35,916 - INFO - Extracting static parameters\n",
            "2025-06-09 04:42:35,916 - INFO - Static parameters extracted:\n",
            "2025-06-09 04:42:35,917 - INFO -   Age range: 0.0 - 62.0\n",
            "2025-06-09 04:42:35,917 - INFO -   Intensity range: 10.0 - 100.0\n",
            "2025-06-09 04:42:35,917 - INFO -   Stimulus rate range: 11.1 - 49.1\n",
            "2025-06-09 04:42:35,918 - INFO -   Hearing loss categories: ['NORMAL', 'SNÄ°K', 'TOTAL', 'Ä°TÄ°K']\n",
            "2025-06-09 04:42:35,918 - INFO - Extracting latency and amplitude data with masking\n",
            "2025-06-09 04:42:35,919 - INFO -   I Latancy: 72/182 available (39.6%)\n",
            "2025-06-09 04:42:35,919 - INFO -   III Latancy: 83/182 available (45.6%)\n",
            "2025-06-09 04:42:35,920 - INFO -   V Latancy: 181/182 available (99.5%)\n",
            "2025-06-09 04:42:35,920 - INFO -   I Amplitude: 72/182 available (39.6%)\n",
            "2025-06-09 04:42:35,921 - INFO -   III Amplitude: 83/182 available (45.6%)\n",
            "2025-06-09 04:42:35,921 - INFO -   V Amplitude: 181/182 available (99.5%)\n",
            "2025-06-09 04:42:35,921 - INFO - Applying data normalization\n",
            "2025-06-09 04:42:35,926 - INFO - Data normalization completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  âœ… Data loaded: 1000 records\n",
            "  âœ… Filters applied: 182 records remaining\n",
            "  âœ… Time series extracted: (182, 200)\n",
            "  âœ… Static parameters extracted: 5 parameters\n",
            "  âœ… Clinical features extracted\n",
            "    Latency features: 3\n",
            "    Amplitude features: 3\n",
            "  âœ… Normalization applied: 10 statistics\n",
            "\n",
            "âœ… Preprocessing pipeline completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Test the current preprocessing pipeline\n",
        "if df is not None:\n",
        "    print(\"ğŸ”§ PREPROCESSING PIPELINE TEST\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Import the preprocessing class\n",
        "        import sys\n",
        "        sys.path.append('..')\n",
        "        from src.data.preprocessing import ABRDataPreprocessor\n",
        "        \n",
        "        # Create a small test with first 1000 rows to speed up analysis\n",
        "        test_df = df.head(1000).copy()\n",
        "        print(f\"ğŸ“Š Testing with subset: {len(test_df)} rows\")\n",
        "        \n",
        "        # Save test data temporarily\n",
        "        test_path = \"temp_test_data.xlsx\"\n",
        "        test_df.to_excel(test_path, index=False)\n",
        "        \n",
        "        # Test preprocessing\n",
        "        preprocessor = ABRDataPreprocessor(test_path, \"temp_output\")\n",
        "        \n",
        "        print(\"\\nğŸ”„ Running preprocessing steps...\")\n",
        "        \n",
        "        # Step 1: Load data\n",
        "        raw_data = preprocessor.load_excel_data()\n",
        "        print(f\"  âœ… Data loaded: {len(raw_data)} records\")\n",
        "        \n",
        "        # Step 2: Apply filters\n",
        "        filtered_data = preprocessor.apply_filters()\n",
        "        print(f\"  âœ… Filters applied: {len(filtered_data)} records remaining\")\n",
        "        \n",
        "        # Step 3: Extract time series\n",
        "        time_series = preprocessor.extract_time_series(200)\n",
        "        print(f\"  âœ… Time series extracted: {time_series.shape}\")\n",
        "        \n",
        "        # Step 4: Extract static parameters\n",
        "        static_params = preprocessor.extract_static_parameters()\n",
        "        print(f\"  âœ… Static parameters extracted: {len(static_params)} parameters\")\n",
        "        \n",
        "        # Step 5: Extract latency/amplitude with masks\n",
        "        clinical_data = preprocessor.extract_latency_amplitude_with_masks()\n",
        "        print(f\"  âœ… Clinical features extracted\")\n",
        "        print(f\"    Latency features: {len(clinical_data['latency_data'])}\")\n",
        "        print(f\"    Amplitude features: {len(clinical_data['amplitude_data'])}\")\n",
        "        \n",
        "        # Step 6: Normalization\n",
        "        norm_stats = preprocessor.normalize_data(True)\n",
        "        print(f\"  âœ… Normalization applied: {len(norm_stats)} statistics\")\n",
        "        \n",
        "        print(f\"\\nâœ… Preprocessing pipeline completed successfully!\")\n",
        "        \n",
        "        # Clean up\n",
        "        import os\n",
        "        if os.path.exists(test_path):\n",
        "            os.remove(test_path)\n",
        "        if os.path.exists(\"temp_output\"):\n",
        "            import shutil\n",
        "            shutil.rmtree(\"temp_output\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Preprocessing pipeline failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Issues Identified and Improvement Suggestions\n",
        "\n",
        "Based on the analysis above, let's summarize the findings and propose improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‹ ANALYSIS SUMMARY AND IMPROVEMENT SUGGESTIONS\n",
            "============================================================\n",
            "\n",
            "ğŸ” ISSUES IDENTIFIED:\n",
            "1. **Column Name Inconsistencies**: Need to verify exact column names\n",
            "2. **Missing Value Handling**: Current approach may be too simplistic\n",
            "3. **Data Validation**: Limited validation of physiological ranges\n",
            "4. **Normalization Strategy**: May not be optimal for all data types\n",
            "5. **Error Handling**: Limited error handling and recovery\n",
            "6. **Memory Efficiency**: Could be optimized for large datasets\n",
            "\n",
            "ğŸ’¡ IMPROVEMENT SUGGESTIONS:\n",
            "\n",
            "1. **Enhanced Column Detection**:\n",
            "   - Implement fuzzy matching for column names\n",
            "   - Add column mapping configuration\n",
            "   - Better handling of variations in column names\n",
            "\n",
            "2. **Improved Missing Value Strategy**:\n",
            "   - Different strategies for different data types\n",
            "   - Consider interpolation for time series\n",
            "   - Advanced imputation methods for clinical features\n",
            "\n",
            "3. **Data Validation Framework**:\n",
            "   - Physiological range validation for latency/amplitude\n",
            "   - Outlier detection and handling\n",
            "   - Data quality scoring\n",
            "\n",
            "4. **Advanced Normalization**:\n",
            "   - Robust normalization (median/MAD)\n",
            "   - Per-group normalization (by hearing loss type)\n",
            "   - Separate normalization for different feature types\n",
            "\n",
            "5. **Better Error Handling**:\n",
            "   - Graceful degradation when columns are missing\n",
            "   - Detailed logging and error reporting\n",
            "   - Recovery mechanisms\n",
            "\n",
            "6. **Performance Optimizations**:\n",
            "   - Chunked processing for large datasets\n",
            "   - Memory-efficient data types\n",
            "   - Parallel processing where applicable\n",
            "\n",
            "7. **Enhanced Validation**:\n",
            "   - Cross-validation of preprocessing steps\n",
            "   - Data integrity checks\n",
            "   - Preprocessing quality metrics\n",
            "\n",
            "ğŸ¯ PRIORITY IMPROVEMENTS:\n",
            "1. Fix column name detection (HIGH)\n",
            "2. Improve missing value handling (HIGH)\n",
            "3. Add data validation (MEDIUM)\n",
            "4. Enhance error handling (MEDIUM)\n",
            "5. Optimize performance (LOW)\n",
            "\n",
            "ğŸ“Š NEXT STEPS:\n",
            "1. Run this notebook to identify specific issues\n",
            "2. Implement priority improvements\n",
            "3. Test with full dataset\n",
            "4. Validate preprocessing quality\n",
            "5. Update documentation\n"
          ]
        }
      ],
      "source": [
        "# Summary of findings and improvement suggestions\n",
        "print(\"ğŸ“‹ ANALYSIS SUMMARY AND IMPROVEMENT SUGGESTIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nğŸ” ISSUES IDENTIFIED:\")\n",
        "print(\"1. **Column Name Inconsistencies**: Need to verify exact column names\")\n",
        "print(\"2. **Missing Value Handling**: Current approach may be too simplistic\")\n",
        "print(\"3. **Data Validation**: Limited validation of physiological ranges\")\n",
        "print(\"4. **Normalization Strategy**: May not be optimal for all data types\")\n",
        "print(\"5. **Error Handling**: Limited error handling and recovery\")\n",
        "print(\"6. **Memory Efficiency**: Could be optimized for large datasets\")\n",
        "\n",
        "print(\"\\nğŸ’¡ IMPROVEMENT SUGGESTIONS:\")\n",
        "\n",
        "print(\"\\n1. **Enhanced Column Detection**:\")\n",
        "print(\"   - Implement fuzzy matching for column names\")\n",
        "print(\"   - Add column mapping configuration\")\n",
        "print(\"   - Better handling of variations in column names\")\n",
        "\n",
        "print(\"\\n2. **Improved Missing Value Strategy**:\")\n",
        "print(\"   - Different strategies for different data types\")\n",
        "print(\"   - Consider interpolation for time series\")\n",
        "print(\"   - Advanced imputation methods for clinical features\")\n",
        "\n",
        "print(\"\\n3. **Data Validation Framework**:\")\n",
        "print(\"   - Physiological range validation for latency/amplitude\")\n",
        "print(\"   - Outlier detection and handling\")\n",
        "print(\"   - Data quality scoring\")\n",
        "\n",
        "print(\"\\n4. **Advanced Normalization**:\")\n",
        "print(\"   - Robust normalization (median/MAD)\")\n",
        "print(\"   - Per-group normalization (by hearing loss type)\")\n",
        "print(\"   - Separate normalization for different feature types\")\n",
        "\n",
        "print(\"\\n5. **Better Error Handling**:\")\n",
        "print(\"   - Graceful degradation when columns are missing\")\n",
        "print(\"   - Detailed logging and error reporting\")\n",
        "print(\"   - Recovery mechanisms\")\n",
        "\n",
        "print(\"\\n6. **Performance Optimizations**:\")\n",
        "print(\"   - Chunked processing for large datasets\")\n",
        "print(\"   - Memory-efficient data types\")\n",
        "print(\"   - Parallel processing where applicable\")\n",
        "\n",
        "print(\"\\n7. **Enhanced Validation**:\")\n",
        "print(\"   - Cross-validation of preprocessing steps\")\n",
        "print(\"   - Data integrity checks\")\n",
        "print(\"   - Preprocessing quality metrics\")\n",
        "\n",
        "print(\"\\nğŸ¯ PRIORITY IMPROVEMENTS:\")\n",
        "print(\"1. Fix column name detection (HIGH)\")\n",
        "print(\"2. Improve missing value handling (HIGH)\")\n",
        "print(\"3. Add data validation (MEDIUM)\")\n",
        "print(\"4. Enhance error handling (MEDIUM)\")\n",
        "print(\"5. Optimize performance (LOW)\")\n",
        "\n",
        "print(\"\\nğŸ“Š NEXT STEPS:\")\n",
        "print(\"1. Run this notebook to identify specific issues\")\n",
        "print(\"2. Implement priority improvements\")\n",
        "print(\"3. Test with full dataset\")\n",
        "print(\"4. Validate preprocessing quality\")\n",
        "print(\"5. Update documentation\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
