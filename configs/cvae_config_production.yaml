# CVAE Production Configuration - Based on Successful V2 Results
# Validation loss improved from 2.06 to 0.40 (80% improvement)
# Reconstruction loss now properly tracked (was 0.0, now ~0.36-0.49)
# KL loss controlled (~74-122 vs previous 467K+)
# Model size reduced from 11.5M to 515K parameters (95% reduction)

# Data configuration - Proven settings
data:
  sequence_length: 200       # ABR waveform length
  channels: 1                # Single channel works well
  train_split: 0.8          # More training data helps
  val_split: 0.1            # Smaller validation set
  test_split: 0.1           # Smaller test set
  random_state: 42          # Reproducibility
  augment_train: true       # Enable light augmentation for production
  augment_prob: 0.2         # Light augmentation
  noise_std: 0.002          # Slightly more noise for robustness
  time_shift_max: 10        # Slightly more time shift

# Model architecture - Proven optimal size
model:
  static_dim: 4             # age, intensity, stimulus_rate, hear_loss
  input_dim: 1              # Single channel
  latent_dim: 32            # Optimal size found
  condition_dim: 32         # Matching latent dim
  hidden_dim: 64            # Optimal hidden size
  num_encoder_layers: 2     # Sufficient depth
  num_decoder_layers: 2     # Matching encoder
  num_heads: 2              # Efficient attention
  dropout: 0.2              # Good regularization
  sequence_length: 200      # Match data
  beta: 1.0                 # Will be overridden by scheduler

# Training configuration - Extended for production
training:
  epochs: 100               # Extended training
  batch_size: 8             # Proven batch size
  initial_beta: 0.0         # Start with no KL penalty
  final_beta: 0.02          # Slightly higher final beta
  beta_annealing_epochs: 80 # Gradual KL introduction
  early_stopping_patience: 25  # Reasonable patience
  checkpoint_interval: 5    # More frequent checkpoints
  sample_interval: 10       # More frequent sampling
  log_interval: 20          # Frequent logging
  output_dir: "outputs"
  
  # Optimizer - Proven settings
  optimizer:
    type: "adam"            # Standard Adam works well
    lr: 0.0001              # Proven learning rate
    weight_decay: 0.0       # No weight decay needed
    betas: [0.9, 0.999]     # Standard betas
    eps: 1e-8
  
  # Learning rate scheduler - Adaptive
  scheduler:
    enabled: true
    type: "plateau"         # Adaptive to validation loss
    factor: 0.7             # Gentle reduction
    patience: 8             # Wait before reducing
    min_lr: 0.000001
  
  # Gradient clipping - Proven settings
  gradient_clipping:
    enabled: true
    max_norm: 0.1           # Tight clipping works well

# Loss configuration - Balanced approach
loss:
  reconstruction_weight: 5.0    # Balanced emphasis
  kl_weight_schedule: "linear"
  kl_warmup_epochs: 60         # Longer warmup
  reconstruction_loss: "mse"
  kl_free_bits: 1.0           # Moderate free bits
  kl_tolerance: 0.3           # Tighter tolerance

# Regularization - Proven effective
regularization:
  spectral_norm: false
  batch_norm: true
  layer_norm: true
  label_smoothing: 0.0      # No label smoothing for regression
  l2_reg: 0.001            # Light L2 regularization

# Evaluation configuration - Production ready
evaluation:
  num_reconstruction_samples: 500
  num_generation_samples: 500
  samples_per_condition: 5
  num_interpolations: 10
  num_latent_samples: 500
  evaluation_interval: 10

# M1 Optimization - Disabled for stability
m1_optimizations:
  use_mps: false
  optimize_memory: false
  use_mixed_precision: false
  compile_model: false

# Advanced training - Production settings
advanced:
  gradient_accumulation_steps: 2  # Effective batch size 16
  ema_decay: 0.999
  use_ema: true                   # Enable EMA for stability
  cyclical_beta: false
  warmup_epochs: 10              # Learning rate warmup

# Debugging and monitoring - Production level
debug:
  log_gradients: false           # Disable for performance
  log_weights: false            # Disable for performance
  check_nan: true               # Keep NaN checking
  gradient_monitoring: false    # Disable for performance
  loss_component_tracking: false # Disable for performance

# Data preprocessing - Production settings
preprocessing:
  normalize_inputs: true
  standardize_targets: true     # Enable for production
  clip_outliers: true          # Enable outlier clipping
  outlier_threshold: 3.0       # Conservative threshold

# Model initialization - Proven settings
initialization:
  weight_init: "xavier_uniform"
  bias_init: "zeros"
  gain: 1.0

# Beta scheduling - Optimized schedule
beta_schedule:
  type: "custom"
  schedule: [
    {"epoch": 0, "beta": 0.0},
    {"epoch": 20, "beta": 0.001},
    {"epoch": 40, "beta": 0.005},
    {"epoch": 60, "beta": 0.01},
    {"epoch": 80, "beta": 0.02}
  ]

# Production monitoring
monitoring:
  save_best_model: true
  save_last_model: true
  save_optimizer_state: true
  track_lr: true
  track_beta: true
  generate_samples: true
  plot_losses: true

# Reproducibility
reproducibility:
  seed: 42
  deterministic: true
  benchmark: false 