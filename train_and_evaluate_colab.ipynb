{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ABR CVAE Training and Evaluation in Google Colab\n",
        "\n",
        "This notebook provides a comprehensive environment for training and evaluating the Conditional Variational Autoencoder (CVAE) for ABR synthetic data generation in Google Colab.\n",
        "\n",
        "## Features:\n",
        "- **Watchdog Integration**: Automatically syncs changes from local files to Colab\n",
        "- **Complete Training Pipeline**: Uses existing train.py module\n",
        "- **Comprehensive Evaluation**: Uses existing evaluate.py module\n",
        "- **Real-time Monitoring**: Training progress visualization\n",
        "- **Model Comparison**: Compare different model architectures\n",
        "- **Google Drive Integration**: Save results to Google Drive\n",
        "\n",
        "## Usage:\n",
        "1. Upload your project files to Google Drive or clone from GitHub\n",
        "2. Run the setup cells to install dependencies and configure watchdog\n",
        "3. Configure training parameters\n",
        "4. Train the model\n",
        "5. Evaluate results\n",
        "\n",
        "Murat\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🔧 Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"🔍 Running in Google Colab\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"🔍 Running locally\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up paths\n",
        "if IN_COLAB:\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Set project path (adjust as needed)\n",
        "    PROJECT_PATH = \"/content/drive/MyDrive/abr-cvae-project\"  # Change this to your project path\n",
        "    \n",
        "    # Alternative: Clone from GitHub\n",
        "    # !git clone https://github.com/your-username/abr-cvae-project.git\n",
        "    # PROJECT_PATH = \"/content/abr-cvae-project\"\n",
        "else:\n",
        "    PROJECT_PATH = os.getcwd()\n",
        "\n",
        "print(f\"📁 Project path: {PROJECT_PATH}\")\n",
        "os.chdir(PROJECT_PATH)\n",
        "sys.path.append(PROJECT_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "if IN_COLAB:\n",
        "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    !pip install watchdog PyYAML scipy scikit-learn matplotlib seaborn tqdm tensorboard openpyxl\n",
        "else:\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install watchdog  # Add watchdog if not in requirements\n",
        "\n",
        "print(\"✅ Dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yaml\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import threading\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"🚀 Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"📊 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(\"✅ Core libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 👀 Watchdog File Synchronization\n",
        "\n",
        "Set up watchdog to automatically sync file changes from your local development environment to Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File synchronization with local watchdog\n",
        "import importlib\n",
        "\n",
        "def check_sync_status():\n",
        "    \"\"\"Check if files are being synced from local machine\"\"\"\n",
        "    sync_indicator = Path(PROJECT_PATH) / \".sync_status\"\n",
        "    \n",
        "    if sync_indicator.exists():\n",
        "        try:\n",
        "            with open(sync_indicator, 'r') as f:\n",
        "                status = f.read().strip()\n",
        "            print(f\"✅ Sync status: {status}\")\n",
        "            return True\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    print(\"⚠️  No sync status found\")\n",
        "    print(\"💡 Make sure to run the local watchdog script:\")\n",
        "    print(f\"   python local_watchdog_sync.py --local-path /path/to/your/project --project-name {os.path.basename(PROJECT_PATH)}\")\n",
        "    return False\n",
        "\n",
        "def reload_modules():\n",
        "    \"\"\"Reload Python modules to get latest changes\"\"\"\n",
        "    modules_to_reload = []\n",
        "    \n",
        "    # Find all imported modules from our project\n",
        "    project_path_obj = Path(PROJECT_PATH)\n",
        "    for module_name, module in sys.modules.items():\n",
        "        if hasattr(module, '__file__') and module.__file__:\n",
        "            try:\n",
        "                module_path = Path(module.__file__)\n",
        "                if project_path_obj in module_path.parents or str(module_path).startswith(str(project_path_obj)):\n",
        "                    modules_to_reload.append(module_name)\n",
        "            except:\n",
        "                continue\n",
        "    \n",
        "    # Reload modules\n",
        "    reloaded_count = 0\n",
        "    for module_name in modules_to_reload:\n",
        "        try:\n",
        "            importlib.reload(sys.modules[module_name])\n",
        "            reloaded_count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Could not reload {module_name}: {e}\")\n",
        "    \n",
        "    if reloaded_count > 0:\n",
        "        print(f\"🔄 Reloaded {reloaded_count} modules\")\n",
        "    else:\n",
        "        print(\"📋 No modules to reload\")\n",
        "    \n",
        "    return reloaded_count\n",
        "\n",
        "def setup_colab_sync():\n",
        "    \"\"\"Setup file synchronization for Colab\"\"\"\n",
        "    print(\"🔧 Setting up file synchronization...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    if IN_COLAB:\n",
        "        print(\"📋 Running in Google Colab\")\n",
        "        print(\"💡 Local Watchdog Setup Instructions:\")\n",
        "        print(\"   1. On your local machine, install watchdog:\")\n",
        "        print(\"      pip install watchdog\")\n",
        "        print(\"   2. Run the sync script:\")\n",
        "        print(f\"      python local_watchdog_sync.py --local-path /path/to/your/project --project-name {os.path.basename(PROJECT_PATH)}\")\n",
        "        print(\"   3. Keep the script running while developing\")\n",
        "        print(\"   4. Your changes will automatically sync to Google Drive\")\n",
        "        print(\"   5. Use reload_modules() in this notebook to get the latest changes\")\n",
        "        \n",
        "        # Check current sync status\n",
        "        print(\"\\n🔍 Checking sync status...\")\n",
        "        check_sync_status()\n",
        "        \n",
        "    else:\n",
        "        print(\"📋 Running locally - no sync needed\")\n",
        "    \n",
        "    print(\"\\n✅ Sync setup complete!\")\n",
        "\n",
        "# Setup sync system\n",
        "setup_colab_sync()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🔄 Module Reloading\n",
        "\n",
        "Use this cell to reload modules after making changes locally. Run this whenever you modify `.py` files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload modules to get latest changes from local development\n",
        "print(\"🔄 Reloading modules...\")\n",
        "reloaded_count = reload_modules()\n",
        "\n",
        "if reloaded_count > 0:\n",
        "    print(f\"✅ Successfully reloaded {reloaded_count} modules\")\n",
        "    print(\"💡 Your latest local changes are now available!\")\n",
        "else:\n",
        "    print(\"📋 No modules needed reloading\")\n",
        "\n",
        "# Check sync status\n",
        "print(\"\\n🔍 Checking sync status...\")\n",
        "check_sync_status()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ⚙️ Configuration Management\n",
        "\n",
        "Load and manage training configurations using the existing YAML configuration files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration management\n",
        "def list_available_configs():\n",
        "    \"\"\"List all available configuration files\"\"\"\n",
        "    config_dir = Path(PROJECT_PATH) / 'configs'\n",
        "    if not config_dir.exists():\n",
        "        print(\"❌ No configs directory found\")\n",
        "        return []\n",
        "    \n",
        "    configs = list(config_dir.glob('*.yaml'))\n",
        "    print(\"📁 Available configurations:\")\n",
        "    for i, config in enumerate(configs):\n",
        "        print(f\"  {i+1}. {config.name}\")\n",
        "    return configs\n",
        "\n",
        "def load_config(config_path):\n",
        "    \"\"\"Load configuration from YAML file\"\"\"\n",
        "    try:\n",
        "        with open(config_path, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "        print(f\"✅ Configuration loaded from {config_path}\")\n",
        "        return config\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading config: {e}\")\n",
        "        return None\n",
        "\n",
        "def display_config(config):\n",
        "    \"\"\"Display configuration in a nice format\"\"\"\n",
        "    print(\"📋 Current Configuration:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for section, values in config.items():\n",
        "        print(f\"\\n🔧 {section.upper()}:\")\n",
        "        if isinstance(values, dict):\n",
        "            for key, value in values.items():\n",
        "                if isinstance(value, dict):\n",
        "                    print(f\"  {key}:\")\n",
        "                    for subkey, subvalue in value.items():\n",
        "                        print(f\"    {subkey}: {subvalue}\")\n",
        "                else:\n",
        "                    print(f\"  {key}: {value}\")\n",
        "        else:\n",
        "            print(f\"  {values}\")\n",
        "\n",
        "def create_colab_config():\n",
        "    \"\"\"Create a Colab-optimized configuration\"\"\"\n",
        "    config = {\n",
        "        'data': {\n",
        "            'sequence_length': 200,\n",
        "            'channels': 1,\n",
        "            'train_split': 0.7,\n",
        "            'val_split': 0.15,\n",
        "            'test_split': 0.15,\n",
        "            'random_state': 42,\n",
        "            'augment_train': True,\n",
        "            'augment_prob': 0.5,\n",
        "            'noise_std': 0.01,\n",
        "            'time_shift_max': 50\n",
        "        },\n",
        "        'model': {\n",
        "            'type': 'original',  # or 'advanced'\n",
        "            'static_dim': 4,\n",
        "            'input_dim': 1,\n",
        "            'latent_dim': 128,\n",
        "            'condition_dim': 128,\n",
        "            'hidden_dim': 256,\n",
        "            'num_encoder_layers': 4,\n",
        "            'num_decoder_layers': 4,\n",
        "            'num_heads': 8,\n",
        "            'dropout': 0.1,\n",
        "            'sequence_length': 200,\n",
        "            'beta': 1.0\n",
        "        },\n",
        "        'training': {\n",
        "            'epochs': 50,  # Reduced for Colab\n",
        "            'batch_size': 16 if torch.cuda.is_available() else 8,  # Colab GPU friendly\n",
        "            'initial_beta': 0.0,\n",
        "            'final_beta': 1.0,\n",
        "            'beta_annealing_epochs': 25,\n",
        "            'early_stopping_patience': 10,\n",
        "            'checkpoint_interval': 5,\n",
        "            'sample_interval': 10,\n",
        "            'log_interval': 50,\n",
        "            'output_dir': 'outputs_colab',\n",
        "            'optimizer': {\n",
        "                'type': 'adamw',\n",
        "                'lr': 0.001,\n",
        "                'weight_decay': 0.0001,\n",
        "                'betas': [0.9, 0.999],\n",
        "                'eps': 1e-8\n",
        "            },\n",
        "            'scheduler': {\n",
        "                'enabled': True,\n",
        "                'type': 'cosine',\n",
        "                'min_lr': 1e-6\n",
        "            },\n",
        "            'gradient_clipping': {\n",
        "                'enabled': True,\n",
        "                'max_norm': 1.0\n",
        "            }\n",
        "        },\n",
        "        'evaluation': {\n",
        "            'num_reconstruction_samples': 500,\n",
        "            'num_generation_samples': 500,\n",
        "            'samples_per_condition': 3,\n",
        "            'num_interpolations': 5,\n",
        "            'num_latent_samples': 500\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return config\n",
        "\n",
        "# List available configurations\n",
        "available_configs = list_available_configs()\n",
        "\n",
        "# Load default configuration or create one for Colab\n",
        "if available_configs:\n",
        "    # Use the first config as default, or let user choose\n",
        "    default_config_path = available_configs[0]\n",
        "    print(f\"\\n🎯 Using default config: {default_config_path.name}\")\n",
        "    config = load_config(default_config_path)\n",
        "else:\n",
        "    print(\"\\n🎯 Creating Colab-optimized configuration\")\n",
        "    config = create_colab_config()\n",
        "\n",
        "if config:\n",
        "    display_config(config)\n",
        "else:\n",
        "    print(\"❌ Failed to load configuration\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🚀 Training Pipeline\n",
        "\n",
        "Use the existing train.py module to train the CVAE model with real-time monitoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training setup and execution\n",
        "import subprocess\n",
        "import threading\n",
        "import queue\n",
        "import re\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class TrainingMonitor:\n",
        "    \"\"\"Monitor training progress in real-time\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.metrics = {\n",
        "            'epochs': [],\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'kl_loss': [],\n",
        "            'recon_loss': [],\n",
        "            'beta': []\n",
        "        }\n",
        "        self.current_epoch = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "        \n",
        "    def parse_log_line(self, line):\n",
        "        \"\"\"Parse training log line and extract metrics\"\"\"\n",
        "        try:\n",
        "            # Extract epoch info\n",
        "            epoch_patterns = [\n",
        "                r'Epoch (\\d+)/(\\d+)',\n",
        "                r'Epoch (\\d+):',\n",
        "                r'Starting epoch (\\d+)'\n",
        "            ]\n",
        "            for pattern in epoch_patterns:\n",
        "                epoch_match = re.search(pattern, line)\n",
        "                if epoch_match:\n",
        "                    self.current_epoch = int(epoch_match.group(1))\n",
        "                    break\n",
        "                \n",
        "            # Extract losses with multiple patterns\n",
        "            train_loss_patterns = [\n",
        "                r'Train Loss: ([\\d.]+)',\n",
        "                r'Training Loss: ([\\d.]+)',\n",
        "                r'train_loss: ([\\d.]+)'\n",
        "            ]\n",
        "            for pattern in train_loss_patterns:\n",
        "                train_loss_match = re.search(pattern, line)\n",
        "                if train_loss_match:\n",
        "                    self.metrics['train_loss'].append(float(train_loss_match.group(1)))\n",
        "                    break\n",
        "                \n",
        "            val_loss_patterns = [\n",
        "                r'Val Loss: ([\\d.]+)',\n",
        "                r'Validation Loss: ([\\d.]+)',\n",
        "                r'val_loss: ([\\d.]+)'\n",
        "            ]\n",
        "            for pattern in val_loss_patterns:\n",
        "                val_loss_match = re.search(pattern, line)\n",
        "                if val_loss_match:\n",
        "                    val_loss = float(val_loss_match.group(1))\n",
        "                    self.metrics['val_loss'].append(val_loss)\n",
        "                    if val_loss < self.best_val_loss:\n",
        "                        self.best_val_loss = val_loss\n",
        "                    break\n",
        "                    \n",
        "            # Extract KL loss\n",
        "            kl_patterns = [\n",
        "                r'KL Loss: ([\\d.]+)',\n",
        "                r'KL: ([\\d.]+)',\n",
        "                r'kl_loss: ([\\d.]+)'\n",
        "            ]\n",
        "            for pattern in kl_patterns:\n",
        "                kl_loss_match = re.search(pattern, line)\n",
        "                if kl_loss_match:\n",
        "                    self.metrics['kl_loss'].append(float(kl_loss_match.group(1)))\n",
        "                    break\n",
        "                    \n",
        "            # Extract reconstruction loss\n",
        "            recon_patterns = [\n",
        "                r'Recon Loss: ([\\d.]+)',\n",
        "                r'Reconstruction Loss: ([\\d.]+)',\n",
        "                r'recon_loss: ([\\d.]+)',\n",
        "                r'Recon: ([\\d.]+)'\n",
        "            ]\n",
        "            for pattern in recon_patterns:\n",
        "                recon_loss_match = re.search(pattern, line)\n",
        "                if recon_loss_match:\n",
        "                    self.metrics['recon_loss'].append(float(recon_loss_match.group(1)))\n",
        "                    break\n",
        "                    \n",
        "            # Extract beta\n",
        "            beta_patterns = [\n",
        "                r'Beta: ([\\d.]+)',\n",
        "                r'beta: ([\\d.]+)'\n",
        "            ]\n",
        "            for pattern in beta_patterns:\n",
        "                beta_match = re.search(pattern, line)\n",
        "                if beta_match:\n",
        "                    self.metrics['beta'].append(float(beta_match.group(1)))\n",
        "                    break\n",
        "                    \n",
        "        except Exception as e:\n",
        "            # Silently continue if parsing fails\n",
        "            pass\n",
        "    \n",
        "    def plot_progress(self):\n",
        "        \"\"\"Plot training progress\"\"\"\n",
        "        if not self.metrics['train_loss']:\n",
        "            return\n",
        "            \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle(f'Training Progress - Epoch {self.current_epoch}', fontsize=16)\n",
        "        \n",
        "        # Loss curves\n",
        "        if self.metrics['train_loss'] and self.metrics['val_loss']:\n",
        "            epochs = range(1, len(self.metrics['train_loss']) + 1)\n",
        "            axes[0, 0].plot(epochs, self.metrics['train_loss'], label='Train Loss', color='blue')\n",
        "            axes[0, 0].plot(epochs, self.metrics['val_loss'], label='Val Loss', color='red')\n",
        "            axes[0, 0].set_title('Training & Validation Loss')\n",
        "            axes[0, 0].set_xlabel('Epoch')\n",
        "            axes[0, 0].set_ylabel('Loss')\n",
        "            axes[0, 0].legend()\n",
        "            axes[0, 0].grid(True)\n",
        "        \n",
        "        # KL and Reconstruction Loss\n",
        "        if self.metrics['kl_loss'] and self.metrics['recon_loss']:\n",
        "            epochs = range(1, len(self.metrics['kl_loss']) + 1)\n",
        "            axes[0, 1].plot(epochs, self.metrics['kl_loss'], label='KL Loss', color='green')\n",
        "            axes[0, 1].plot(epochs, self.metrics['recon_loss'], label='Recon Loss', color='orange')\n",
        "            axes[0, 1].set_title('KL & Reconstruction Loss')\n",
        "            axes[0, 1].set_xlabel('Epoch')\n",
        "            axes[0, 1].set_ylabel('Loss')\n",
        "            axes[0, 1].legend()\n",
        "            axes[0, 1].grid(True)\n",
        "        \n",
        "        # Beta annealing\n",
        "        if self.metrics['beta']:\n",
        "            epochs = range(1, len(self.metrics['beta']) + 1)\n",
        "            axes[1, 0].plot(epochs, self.metrics['beta'], label='Beta', color='purple')\n",
        "            axes[1, 0].set_title('Beta Annealing')\n",
        "            axes[1, 0].set_xlabel('Epoch')\n",
        "            axes[1, 0].set_ylabel('Beta Value')\n",
        "            axes[1, 0].legend()\n",
        "            axes[1, 0].grid(True)\n",
        "        \n",
        "        # Summary stats\n",
        "        axes[1, 1].axis('off')\n",
        "        \n",
        "        # Format stats with proper conditional handling\n",
        "        latest_train = f\"{self.metrics['train_loss'][-1]:.4f}\" if self.metrics['train_loss'] else 'N/A'\n",
        "        latest_val = f\"{self.metrics['val_loss'][-1]:.4f}\" if self.metrics['val_loss'] else 'N/A'\n",
        "        current_beta = f\"{self.metrics['beta'][-1]:.4f}\" if self.metrics['beta'] else 'N/A'\n",
        "        \n",
        "        stats_text = f\"\"\"\n",
        "        Current Epoch: {self.current_epoch}\n",
        "        Best Val Loss: {self.best_val_loss:.4f}\n",
        "        Latest Train Loss: {latest_train}\n",
        "        Latest Val Loss: {latest_val}\n",
        "        Current Beta: {current_beta}\n",
        "        \"\"\"\n",
        "        axes[1, 1].text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def print_summary(self):\n",
        "        \"\"\"Print a simple text summary of training progress\"\"\"\n",
        "        print(f\"📊 Training Summary:\")\n",
        "        print(f\"   Current Epoch: {self.current_epoch}\")\n",
        "        print(f\"   Best Val Loss: {self.best_val_loss:.4f}\")\n",
        "        if self.metrics['train_loss']:\n",
        "            print(f\"   Latest Train Loss: {self.metrics['train_loss'][-1]:.4f}\")\n",
        "        if self.metrics['val_loss']:\n",
        "            print(f\"   Latest Val Loss: {self.metrics['val_loss'][-1]:.4f}\")\n",
        "        if self.metrics['beta']:\n",
        "            print(f\"   Current Beta: {self.metrics['beta'][-1]:.4f}\")\n",
        "        print(f\"   Total Epochs Logged: {len(self.metrics['train_loss'])}\")\n",
        "\n",
        "def run_training_with_monitoring(config, output_dir=\"outputs_colab\"):\n",
        "    \"\"\"Run training with real-time monitoring\"\"\"\n",
        "    \n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Save config for training\n",
        "    config_path = os.path.join(output_dir, \"colab_config.yaml\")\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(config, f, default_flow_style=False)\n",
        "    \n",
        "    # Initialize monitor\n",
        "    monitor = TrainingMonitor()\n",
        "    \n",
        "    # Prepare training command\n",
        "    cmd = [\n",
        "        sys.executable, \"train.py\",\n",
        "        \"--config\", config_path,\n",
        "        \"--output-dir\", output_dir,\n",
        "        \"--device\", str(device)\n",
        "    ]\n",
        "    \n",
        "    print(f\"🚀 Starting training with command: {' '.join(cmd)}\")\n",
        "    print(\"📊 Training progress will be displayed below...\")\n",
        "    \n",
        "    # Run training process\n",
        "    try:\n",
        "        process = subprocess.Popen(\n",
        "            cmd,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            universal_newlines=True,\n",
        "            bufsize=1\n",
        "        )\n",
        "        \n",
        "        # Monitor output in real-time\n",
        "        line_count = 0\n",
        "        last_plot_update = 0\n",
        "        \n",
        "        for line in iter(process.stdout.readline, ''):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                line_count += 1\n",
        "                \n",
        "                # Only print important lines to avoid spam\n",
        "                if any(keyword in line for keyword in [\n",
        "                    'Epoch', 'Loss:', 'Starting', 'Model', 'Training', 'Validation', \n",
        "                    'Best', 'Saved', 'Early stopping', 'completed', 'ERROR', 'WARNING'\n",
        "                ]):\n",
        "                    print(line)\n",
        "                \n",
        "                # Parse metrics from all lines\n",
        "                monitor.parse_log_line(line)\n",
        "                \n",
        "                # Update plots less frequently to avoid spam\n",
        "                if ('Validation Loss:' in line or 'Val Loss:' in line) and (line_count - last_plot_update > 50):\n",
        "                    last_plot_update = line_count\n",
        "                    clear_output(wait=True)\n",
        "                    print(\"🔄 Updating training progress...\")\n",
        "                    monitor.print_summary()\n",
        "                    monitor.plot_progress()\n",
        "                \n",
        "                # Show progress every 200 lines for very verbose output  \n",
        "                elif line_count % 200 == 0:\n",
        "                    print(f\"📊 Training in progress... ({line_count} lines processed)\")\n",
        "                    if monitor.current_epoch > 0:\n",
        "                        monitor.print_summary()\n",
        "        \n",
        "        # Final plot update\n",
        "        if monitor.metrics['train_loss']:\n",
        "            clear_output(wait=True)\n",
        "            print(\"🎯 Training completed! Final results:\")\n",
        "            monitor.plot_progress()\n",
        "        \n",
        "        # Wait for process to complete\n",
        "        return_code = process.wait()\n",
        "        \n",
        "        if return_code == 0:\n",
        "            print(\"✅ Training completed successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"❌ Training failed with return code: {return_code}\")\n",
        "            return False\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during training: {e}\")\n",
        "        return False\n",
        "\n",
        "# Training execution\n",
        "def start_training():\n",
        "    \"\"\"Start the training process\"\"\"\n",
        "    if not config:\n",
        "        print(\"❌ No configuration loaded. Please run the configuration cell first.\")\n",
        "        return False\n",
        "    \n",
        "    print(\"🎯 Starting CVAE training...\")\n",
        "    print(f\"📋 Model type: {config['model'].get('type', 'original')}\")\n",
        "    print(f\"📋 Epochs: {config['training']['epochs']}\")\n",
        "    print(f\"📋 Batch size: {config['training']['batch_size']}\")\n",
        "    print(f\"📋 Device: {device}\")\n",
        "    \n",
        "    # Ensure data directory exists\n",
        "    data_dir = os.path.join(PROJECT_PATH, \"data\", \"processed\")\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"❌ Data directory not found: {data_dir}\")\n",
        "        print(\"📋 Please ensure your preprocessed data is available.\")\n",
        "        return False\n",
        "    \n",
        "    return run_training_with_monitoring(config)\n",
        "\n",
        "print(\"✅ Training pipeline ready!\")\n",
        "print(\"📋 Run start_training() to begin training\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🧪 Evaluation Pipeline\n",
        "\n",
        "Use the existing evaluate.py module to comprehensively evaluate the trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation setup and execution\n",
        "def find_best_checkpoint(output_dir=\"outputs_colab\"):\n",
        "    \"\"\"Find the best checkpoint from training\"\"\"\n",
        "    output_path = Path(output_dir)\n",
        "    \n",
        "    if not output_path.exists():\n",
        "        print(f\"❌ Output directory not found: {output_dir}\")\n",
        "        return None\n",
        "    \n",
        "    # Look for best checkpoint\n",
        "    best_checkpoint = output_path / \"best_checkpoint.pth\"\n",
        "    if best_checkpoint.exists():\n",
        "        print(f\"✅ Found best checkpoint: {best_checkpoint}\")\n",
        "        return str(best_checkpoint)\n",
        "    \n",
        "    # Look for any checkpoint files\n",
        "    checkpoints = list(output_path.glob(\"checkpoint_epoch_*.pth\"))\n",
        "    if checkpoints:\n",
        "        # Return the latest checkpoint\n",
        "        latest_checkpoint = max(checkpoints, key=lambda p: p.stat().st_mtime)\n",
        "        print(f\"✅ Found latest checkpoint: {latest_checkpoint}\")\n",
        "        return str(latest_checkpoint)\n",
        "    \n",
        "    print(\"❌ No checkpoint files found\")\n",
        "    return None\n",
        "\n",
        "def run_evaluation(checkpoint_path, output_dir=\"results_colab\"):\n",
        "    \"\"\"Run comprehensive evaluation using evaluate.py\"\"\"\n",
        "    \n",
        "    if not checkpoint_path or not os.path.exists(checkpoint_path):\n",
        "        print(f\"❌ Checkpoint not found: {checkpoint_path}\")\n",
        "        return False\n",
        "    \n",
        "    # Create results directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Prepare evaluation command\n",
        "    cmd = [\n",
        "        sys.executable, \"evaluate.py\",\n",
        "        \"--model\", checkpoint_path,\n",
        "        \"--output-dir\", output_dir,\n",
        "        \"--comprehensive\"\n",
        "    ]\n",
        "    \n",
        "    print(f\"🔬 Starting evaluation with command: {' '.join(cmd)}\")\n",
        "    print(\"📊 Evaluation progress will be displayed below...\")\n",
        "    \n",
        "    try:\n",
        "        # Run evaluation process\n",
        "        result = subprocess.run(\n",
        "            cmd,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=3600  # 1 hour timeout\n",
        "        )\n",
        "        \n",
        "        # Display output\n",
        "        if result.stdout:\n",
        "            print(\"📋 Evaluation Output:\")\n",
        "            print(result.stdout)\n",
        "        \n",
        "        if result.stderr:\n",
        "            print(\"⚠️ Evaluation Warnings/Errors:\")\n",
        "            print(result.stderr)\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            print(\"✅ Evaluation completed successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"❌ Evaluation failed with return code: {result.returncode}\")\n",
        "            return False\n",
        "            \n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"❌ Evaluation timed out after 1 hour\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during evaluation: {e}\")\n",
        "        return False\n",
        "\n",
        "def display_evaluation_results(results_dir=\"results_colab\"):\n",
        "    \"\"\"Display evaluation results\"\"\"\n",
        "    results_path = Path(results_dir)\n",
        "    \n",
        "    if not results_path.exists():\n",
        "        print(f\"❌ Results directory not found: {results_dir}\")\n",
        "        return\n",
        "    \n",
        "    print(\"📊 Evaluation Results:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Look for evaluation subdirectories\n",
        "    eval_dirs = [d for d in results_path.iterdir() if d.is_dir() and d.name.startswith('eval_')]\n",
        "    \n",
        "    if not eval_dirs:\n",
        "        print(\"❌ No evaluation results found\")\n",
        "        return\n",
        "    \n",
        "    # Use the most recent evaluation\n",
        "    latest_eval_dir = max(eval_dirs, key=lambda d: d.stat().st_mtime)\n",
        "    print(f\"📁 Latest evaluation: {latest_eval_dir.name}\")\n",
        "    \n",
        "    # Display metadata\n",
        "    metadata_file = latest_eval_dir / \"evaluation_metadata.json\"\n",
        "    if metadata_file.exists():\n",
        "        with open(metadata_file, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "        print(\"\\\\n📋 Evaluation Metadata:\")\n",
        "        for key, value in metadata.items():\n",
        "            if isinstance(value, dict):\n",
        "                print(f\"  {key}:\")\n",
        "                for subkey, subvalue in value.items():\n",
        "                    print(f\"    {subkey}: {subvalue}\")\n",
        "            else:\n",
        "                print(f\"  {key}: {value}\")\n",
        "    \n",
        "    # Look for generated plots\n",
        "    plot_files = list(latest_eval_dir.glob(\"*.png\"))\n",
        "    if plot_files:\n",
        "        print(f\"\\\\n🖼️ Generated plots ({len(plot_files)} files):\")\n",
        "        for plot_file in plot_files[:5]:  # Show first 5\n",
        "            print(f\"  📊 {plot_file.name}\")\n",
        "    \n",
        "    # Look for CSV results\n",
        "    csv_files = list(latest_eval_dir.glob(\"*.csv\"))\n",
        "    if csv_files:\n",
        "        print(f\"\\\\n📈 CSV results ({len(csv_files)} files):\")\n",
        "        for csv_file in csv_files:\n",
        "            print(f\"  📄 {csv_file.name}\")\n",
        "            \n",
        "            # Display first few rows if it's a small file\n",
        "            try:\n",
        "                df = pd.read_csv(csv_file)\n",
        "                if len(df) <= 10:\n",
        "                    print(f\"    Preview:\")\n",
        "                    print(df.to_string(index=False, max_rows=5))\n",
        "                else:\n",
        "                    print(f\"    Shape: {df.shape}\")\n",
        "                    print(f\"    Columns: {list(df.columns)}\")\n",
        "            except Exception as e:\n",
        "                print(f\"    Error reading CSV: {e}\")\n",
        "\n",
        "def start_evaluation():\n",
        "    \"\"\"Start the evaluation process\"\"\"\n",
        "    \n",
        "    # Find the best checkpoint\n",
        "    checkpoint_path = find_best_checkpoint()\n",
        "    \n",
        "    if not checkpoint_path:\n",
        "        print(\"❌ No checkpoint found. Please ensure training has completed.\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"🎯 Starting evaluation of checkpoint: {checkpoint_path}\")\n",
        "    \n",
        "    # Run evaluation\n",
        "    success = run_evaluation(checkpoint_path)\n",
        "    \n",
        "    if success:\n",
        "        print(\"\\\\n📊 Displaying evaluation results...\")\n",
        "        display_evaluation_results()\n",
        "    \n",
        "    return success\n",
        "\n",
        "print(\"✅ Evaluation pipeline ready!\")\n",
        "print(\"📋 Run start_evaluation() to evaluate the trained model\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📊 Quick Training Execution\n",
        "\n",
        "Run this cell to start training immediately with default settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick training execution\n",
        "print(\"🚀 Starting training with current configuration...\")\n",
        "training_success = start_training()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🧪 Quick Evaluation Execution\n",
        "\n",
        "Run this cell to evaluate the trained model immediately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick evaluation execution\n",
        "print(\"🔬 Starting evaluation of trained model...\")\n",
        "evaluation_success = start_evaluation()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🔧 Advanced Configuration\n",
        "\n",
        "Customize training parameters and model architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced configuration options\n",
        "def create_custom_config():\n",
        "    \"\"\"Create custom configuration with advanced options\"\"\"\n",
        "    \n",
        "    print(\"🔧 Custom Configuration Builder\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Model type selection\n",
        "    print(\"\\\\n1. Model Type:\")\n",
        "    print(\"   a) Original CVAE\")\n",
        "    print(\"   b) Advanced CVAE (hierarchical latents)\")\n",
        "    \n",
        "    model_type = input(\"Choose model type (a/b): \").lower()\n",
        "    if model_type == 'b':\n",
        "        model_type = 'advanced'\n",
        "    else:\n",
        "        model_type = 'original'\n",
        "    \n",
        "    # Training parameters\n",
        "    print(\"\\\\n2. Training Parameters:\")\n",
        "    epochs = int(input(\"Number of epochs (default 50): \") or \"50\")\n",
        "    batch_size = int(input(\"Batch size (default 16): \") or \"16\")\n",
        "    learning_rate = float(input(\"Learning rate (default 0.001): \") or \"0.001\")\n",
        "    \n",
        "    # Model architecture\n",
        "    print(\"\\\\n3. Model Architecture:\")\n",
        "    latent_dim = int(input(\"Latent dimension (default 128): \") or \"128\")\n",
        "    hidden_dim = int(input(\"Hidden dimension (default 256): \") or \"256\")\n",
        "    \n",
        "    # Create custom config\n",
        "    custom_config = {\n",
        "        'data': {\n",
        "            'sequence_length': 200,\n",
        "            'channels': 1,\n",
        "            'train_split': 0.7,\n",
        "            'val_split': 0.15,\n",
        "            'test_split': 0.15,\n",
        "            'random_state': 42,\n",
        "            'augment_train': True,\n",
        "            'augment_prob': 0.5,\n",
        "            'noise_std': 0.01,\n",
        "            'time_shift_max': 50\n",
        "        },\n",
        "        'model': {\n",
        "            'type': model_type,\n",
        "            'static_dim': 4,\n",
        "            'input_dim': 1,\n",
        "            'latent_dim': latent_dim,\n",
        "            'condition_dim': 128,\n",
        "            'hidden_dim': hidden_dim,\n",
        "            'num_encoder_layers': 4,\n",
        "            'num_decoder_layers': 4,\n",
        "            'num_heads': 8,\n",
        "            'dropout': 0.1,\n",
        "            'sequence_length': 200,\n",
        "            'beta': 1.0\n",
        "        },\n",
        "        'training': {\n",
        "            'epochs': epochs,\n",
        "            'batch_size': batch_size,\n",
        "            'initial_beta': 0.0,\n",
        "            'final_beta': 1.0,\n",
        "            'beta_annealing_epochs': epochs // 2,\n",
        "            'early_stopping_patience': max(10, epochs // 5),\n",
        "            'checkpoint_interval': max(5, epochs // 10),\n",
        "            'sample_interval': max(10, epochs // 5),\n",
        "            'log_interval': 50,\n",
        "            'output_dir': 'outputs_custom',\n",
        "            'optimizer': {\n",
        "                'type': 'adamw',\n",
        "                'lr': learning_rate,\n",
        "                'weight_decay': 0.0001,\n",
        "                'betas': [0.9, 0.999],\n",
        "                'eps': 1e-8\n",
        "            },\n",
        "            'scheduler': {\n",
        "                'enabled': True,\n",
        "                'type': 'cosine',\n",
        "                'min_lr': 1e-6\n",
        "            },\n",
        "            'gradient_clipping': {\n",
        "                'enabled': True,\n",
        "                'max_norm': 1.0\n",
        "            }\n",
        "        },\n",
        "        'evaluation': {\n",
        "            'num_reconstruction_samples': 500,\n",
        "            'num_generation_samples': 500,\n",
        "            'samples_per_condition': 3,\n",
        "            'num_interpolations': 5,\n",
        "            'num_latent_samples': 500\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(\"\\\\n✅ Custom configuration created!\")\n",
        "    return custom_config\n",
        "\n",
        "def compare_models():\n",
        "    \"\"\"Compare different model configurations\"\"\"\n",
        "    \n",
        "    # Original model config\n",
        "    original_config = create_colab_config()\n",
        "    original_config['model']['type'] = 'original'\n",
        "    original_config['training']['output_dir'] = 'outputs_original'\n",
        "    \n",
        "    # Advanced model config\n",
        "    advanced_config = create_colab_config()\n",
        "    advanced_config['model']['type'] = 'advanced'\n",
        "    advanced_config['training']['output_dir'] = 'outputs_advanced'\n",
        "    \n",
        "    configs = {\n",
        "        'Original CVAE': original_config,\n",
        "        'Advanced CVAE': advanced_config\n",
        "    }\n",
        "    \n",
        "    print(\"🔬 Model Comparison Mode\")\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for model_name, model_config in configs.items():\n",
        "        print(f\"\\\\n🚀 Training {model_name}...\")\n",
        "        \n",
        "        # Update global config\n",
        "        global config\n",
        "        config = model_config\n",
        "        \n",
        "        # Train model\n",
        "        success = run_training_with_monitoring(model_config, model_config['training']['output_dir'])\n",
        "        \n",
        "        if success:\n",
        "            print(f\"✅ {model_name} training completed!\")\n",
        "            \n",
        "            # Evaluate model\n",
        "            checkpoint_path = find_best_checkpoint(model_config['training']['output_dir'])\n",
        "            if checkpoint_path:\n",
        "                eval_success = run_evaluation(checkpoint_path, f\"results_{model_name.lower().replace(' ', '_')}\")\n",
        "                results[model_name] = {\n",
        "                    'training_success': True,\n",
        "                    'evaluation_success': eval_success,\n",
        "                    'checkpoint_path': checkpoint_path\n",
        "                }\n",
        "            else:\n",
        "                results[model_name] = {\n",
        "                    'training_success': True,\n",
        "                    'evaluation_success': False,\n",
        "                    'checkpoint_path': None\n",
        "                }\n",
        "        else:\n",
        "            print(f\"❌ {model_name} training failed!\")\n",
        "            results[model_name] = {\n",
        "                'training_success': False,\n",
        "                'evaluation_success': False,\n",
        "                'checkpoint_path': None\n",
        "            }\n",
        "    \n",
        "    # Display comparison results\n",
        "    print(\"\\\\n📊 Model Comparison Results:\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    for model_name, result in results.items():\n",
        "        print(f\"\\\\n🔬 {model_name}:\")\n",
        "        print(f\"  Training: {'✅' if result['training_success'] else '❌'}\")\n",
        "        print(f\"  Evaluation: {'✅' if result['evaluation_success'] else '❌'}\")\n",
        "        if result['checkpoint_path']:\n",
        "            print(f\"  Checkpoint: {result['checkpoint_path']}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"✅ Advanced configuration tools ready!\")\n",
        "print(\"📋 Available functions:\")\n",
        "print(\"  - create_custom_config(): Build custom configuration\")\n",
        "print(\"  - compare_models(): Compare original vs advanced models\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 💾 Google Drive Integration\n",
        "\n",
        "Save results and checkpoints to Google Drive for persistence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Drive integration\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "def backup_to_drive():\n",
        "    \"\"\"Backup training results to Google Drive\"\"\"\n",
        "    \n",
        "    if not IN_COLAB:\n",
        "        print(\"📋 Drive backup is only available in Google Colab\")\n",
        "        return\n",
        "    \n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_dir = f\"/content/drive/MyDrive/abr_cvae_backups/backup_{timestamp}\"\n",
        "    \n",
        "    print(f\"💾 Creating backup in Google Drive: {backup_dir}\")\n",
        "    \n",
        "    try:\n",
        "        os.makedirs(backup_dir, exist_ok=True)\n",
        "        \n",
        "        # Backup outputs\n",
        "        output_dirs = [\"outputs_colab\", \"outputs_original\", \"outputs_advanced\", \"outputs_custom\"]\n",
        "        for output_dir in output_dirs:\n",
        "            if os.path.exists(output_dir):\n",
        "                print(f\"📁 Backing up {output_dir}...\")\n",
        "                shutil.copytree(output_dir, os.path.join(backup_dir, output_dir))\n",
        "        \n",
        "        # Backup results\n",
        "        result_dirs = [\"results_colab\", \"results_original_cvae\", \"results_advanced_cvae\"]\n",
        "        for result_dir in result_dirs:\n",
        "            if os.path.exists(result_dir):\n",
        "                print(f\"📊 Backing up {result_dir}...\")\n",
        "                shutil.copytree(result_dir, os.path.join(backup_dir, result_dir))\n",
        "        \n",
        "        # Create backup metadata\n",
        "        metadata = {\n",
        "            \"backup_timestamp\": timestamp,\n",
        "            \"project_path\": PROJECT_PATH,\n",
        "            \"device_used\": str(device),\n",
        "            \"torch_version\": torch.__version__,\n",
        "            \"python_version\": sys.version\n",
        "        }\n",
        "        \n",
        "        metadata_path = os.path.join(backup_dir, \"backup_metadata.json\")\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        \n",
        "        print(f\"✅ Backup completed successfully!\")\n",
        "        print(f\"📁 Backup location: {backup_dir}\")\n",
        "        \n",
        "        return backup_dir\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Backup failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def restore_from_drive():\n",
        "    \"\"\"Restore training results from Google Drive\"\"\"\n",
        "    \n",
        "    if not IN_COLAB:\n",
        "        print(\"📋 Drive restore is only available in Google Colab\")\n",
        "        return\n",
        "    \n",
        "    backup_base = \"/content/drive/MyDrive/abr_cvae_backups\"\n",
        "    \n",
        "    if not os.path.exists(backup_base):\n",
        "        print(\"❌ No backups found in Google Drive\")\n",
        "        return\n",
        "    \n",
        "    # List available backups\n",
        "    backups = [d for d in os.listdir(backup_base) if d.startswith(\"backup_\")]\n",
        "    backups.sort(reverse=True)  # Most recent first\n",
        "    \n",
        "    if not backups:\n",
        "        print(\"❌ No backups found\")\n",
        "        return\n",
        "    \n",
        "    print(\"📁 Available backups:\")\n",
        "    for i, backup in enumerate(backups[:5]):  # Show last 5 backups\n",
        "        print(f\"  {i+1}. {backup}\")\n",
        "    \n",
        "    try:\n",
        "        choice = int(input(\"Choose backup to restore (1-5): \")) - 1\n",
        "        if 0 <= choice < len(backups):\n",
        "            backup_path = os.path.join(backup_base, backups[choice])\n",
        "            \n",
        "            print(f\"🔄 Restoring from: {backup_path}\")\n",
        "            \n",
        "            # Restore directories\n",
        "            for item in os.listdir(backup_path):\n",
        "                item_path = os.path.join(backup_path, item)\n",
        "                if os.path.isdir(item_path) and not item.endswith('.json'):\n",
        "                    print(f\"📁 Restoring {item}...\")\n",
        "                    if os.path.exists(item):\n",
        "                        shutil.rmtree(item)\n",
        "                    shutil.copytree(item_path, item)\n",
        "            \n",
        "            print(\"✅ Restore completed successfully!\")\n",
        "            \n",
        "        else:\n",
        "            print(\"❌ Invalid choice\")\n",
        "            \n",
        "    except ValueError:\n",
        "        print(\"❌ Invalid input\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Restore failed: {e}\")\n",
        "\n",
        "def sync_to_drive(source_dir, drive_subdir=\"abr_cvae_sync\"):\n",
        "    \"\"\"Sync specific directory to Google Drive\"\"\"\n",
        "    \n",
        "    if not IN_COLAB:\n",
        "        print(\"📋 Drive sync is only available in Google Colab\")\n",
        "        return\n",
        "    \n",
        "    if not os.path.exists(source_dir):\n",
        "        print(f\"❌ Source directory not found: {source_dir}\")\n",
        "        return\n",
        "    \n",
        "    drive_path = f\"/content/drive/MyDrive/{drive_subdir}\"\n",
        "    sync_path = os.path.join(drive_path, os.path.basename(source_dir))\n",
        "    \n",
        "    try:\n",
        "        os.makedirs(drive_path, exist_ok=True)\n",
        "        \n",
        "        print(f\"🔄 Syncing {source_dir} to {sync_path}...\")\n",
        "        \n",
        "        if os.path.exists(sync_path):\n",
        "            shutil.rmtree(sync_path)\n",
        "        \n",
        "        shutil.copytree(source_dir, sync_path)\n",
        "        \n",
        "        print(f\"✅ Sync completed: {sync_path}\")\n",
        "        return sync_path\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Sync failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Auto-backup function\n",
        "def setup_auto_backup():\n",
        "    \"\"\"Setup automatic backup every hour\"\"\"\n",
        "    \n",
        "    if not IN_COLAB:\n",
        "        print(\"📋 Auto-backup is only available in Google Colab\")\n",
        "        return\n",
        "    \n",
        "    def backup_worker():\n",
        "        while True:\n",
        "            time.sleep(3600)  # Wait 1 hour\n",
        "            print(\"⏰ Performing automatic backup...\")\n",
        "            backup_to_drive()\n",
        "    \n",
        "    backup_thread = threading.Thread(target=backup_worker, daemon=True)\n",
        "    backup_thread.start()\n",
        "    \n",
        "    print(\"⏰ Auto-backup enabled (every hour)\")\n",
        "\n",
        "print(\"✅ Google Drive integration ready!\")\n",
        "print(\"📋 Available functions:\")\n",
        "print(\"  - backup_to_drive(): Backup all results to Google Drive\")\n",
        "print(\"  - restore_from_drive(): Restore from previous backup\")\n",
        "print(\"  - sync_to_drive(dir): Sync specific directory to Drive\")\n",
        "print(\"  - setup_auto_backup(): Enable automatic hourly backups\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🎯 Summary and Next Steps\n",
        "\n",
        "This notebook provides a complete pipeline for training and evaluating ABR CVAE models in Google Colab with watchdog integration.\n",
        "\n",
        "### Key Features:\n",
        "1. **Watchdog Integration**: Automatically syncs file changes from your local environment\n",
        "2. **Modular Design**: Uses existing `.py` files without modification\n",
        "3. **Real-time Monitoring**: Training progress visualization\n",
        "4. **Comprehensive Evaluation**: Complete model assessment\n",
        "5. **Google Drive Integration**: Automatic backup and sync\n",
        "6. **Model Comparison**: Compare different architectures\n",
        "\n",
        "### Workflow:\n",
        "1. **Setup**: Run setup cells to configure environment and watchdog\n",
        "2. **Configuration**: Load or create training configuration\n",
        "3. **Training**: Execute training with real-time monitoring\n",
        "4. **Evaluation**: Comprehensive model evaluation\n",
        "5. **Backup**: Save results to Google Drive\n",
        "\n",
        "### Advanced Features:\n",
        "- Custom configuration builder\n",
        "- Model comparison utilities\n",
        "- Automatic backup system\n",
        "- File synchronization with Drive\n",
        "\n",
        "### Tips for Using with Watchdog:\n",
        "1. **Local Development**: Make changes to your `.py` files locally\n",
        "2. **Automatic Sync**: Watchdog will detect changes and reload modules\n",
        "3. **Drive Sync**: Ensure your project is synced to Google Drive\n",
        "4. **Backup Regularly**: Use the backup functions to save progress\n",
        "\n",
        "### Troubleshooting:\n",
        "- Ensure all `.py` files are in the correct directory structure\n",
        "- Check that preprocessed data is available in `data/processed/`\n",
        "- Monitor GPU memory usage in Colab\n",
        "- Use the backup functions to save work periodically\n",
        "\n",
        "**Ready to start training! 🚀**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "abr_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
