{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ABR CVAE Training and Evaluation in Google Colab\n",
        "\n",
        "This notebook provides a comprehensive environment for training and evaluating the Conditional Variational Autoencoder (CVAE) for ABR synthetic data generation in Google Colab.\n",
        "\n",
        "## Features:\n",
        "- **Watchdog Integration**: Automatically syncs changes from local files to Colab\n",
        "- **Complete Training Pipeline**: Uses existing train.py module\n",
        "- **Comprehensive Evaluation**: Uses existing evaluate.py module\n",
        "- **Real-time Monitoring**: Training progress visualization\n",
        "- **Model Comparison**: Compare different model architectures\n",
        "- **Google Drive Integration**: Save results to Google Drive\n",
        "\n",
        "## Usage:\n",
        "1. Upload your project files to Google Drive or clone from GitHub\n",
        "2. Run the setup cells to install dependencies and configure watchdog\n",
        "3. Configure training parameters\n",
        "4. Train the model\n",
        "5. Evaluate results\n",
        "\n",
        "Murat\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"üîç Running in Google Colab\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"üîç Running locally\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up paths\n",
        "if IN_COLAB:\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Set project path (adjust as needed)\n",
        "    PROJECT_PATH = \"/content/drive/MyDrive/abr-cvae-project\"  # Change this to your project path\n",
        "    \n",
        "    # Alternative: Clone from GitHub\n",
        "    # !git clone https://github.com/your-username/abr-cvae-project.git\n",
        "    # PROJECT_PATH = \"/content/abr-cvae-project\"\n",
        "else:\n",
        "    PROJECT_PATH = os.getcwd()\n",
        "\n",
        "print(f\"üìÅ Project path: {PROJECT_PATH}\")\n",
        "os.chdir(PROJECT_PATH)\n",
        "sys.path.append(PROJECT_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "if IN_COLAB:\n",
        "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    !pip install watchdog PyYAML scipy scikit-learn matplotlib seaborn tqdm tensorboard openpyxl\n",
        "else:\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install watchdog  # Add watchdog if not in requirements\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yaml\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import threading\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üöÄ Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(\"‚úÖ Core libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üëÄ Watchdog File Synchronization\n",
        "\n",
        "Set up watchdog to automatically sync file changes from your local development environment to Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File synchronization with local watchdog\n",
        "import importlib\n",
        "\n",
        "def check_sync_status():\n",
        "    \"\"\"Check if files are being synced from local machine\"\"\"\n",
        "    sync_indicator = Path(PROJECT_PATH) / \".sync_status\"\n",
        "    \n",
        "    if sync_indicator.exists():\n",
        "        try:\n",
        "            with open(sync_indicator, 'r') as f:\n",
        "                status = f.read().strip()\n",
        "            print(f\"‚úÖ Sync status: {status}\")\n",
        "            return True\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    print(\"‚ö†Ô∏è  No sync status found\")\n",
        "    print(\"üí° Make sure to run the local watchdog script:\")\n",
        "    print(f\"   python local_watchdog_sync.py --local-path /path/to/your/project --project-name {os.path.basename(PROJECT_PATH)}\")\n",
        "    return False\n",
        "\n",
        "def reload_modules():\n",
        "    \"\"\"Reload Python modules to get latest changes\"\"\"\n",
        "    modules_to_reload = []\n",
        "    \n",
        "    # Find all imported modules from our project\n",
        "    project_path_obj = Path(PROJECT_PATH)\n",
        "    for module_name, module in sys.modules.items():\n",
        "        if hasattr(module, '__file__') and module.__file__:\n",
        "            try:\n",
        "                module_path = Path(module.__file__)\n",
        "                if project_path_obj in module_path.parents or str(module_path).startswith(str(project_path_obj)):\n",
        "                    modules_to_reload.append(module_name)\n",
        "            except:\n",
        "                continue\n",
        "    \n",
        "    # Reload modules\n",
        "    reloaded_count = 0\n",
        "    for module_name in modules_to_reload:\n",
        "        try:\n",
        "            importlib.reload(sys.modules[module_name])\n",
        "            reloaded_count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Could not reload {module_name}: {e}\")\n",
        "    \n",
        "    if reloaded_count > 0:\n",
        "        print(f\"üîÑ Reloaded {reloaded_count} modules\")\n",
        "    else:\n",
        "        print(\"üìã No modules to reload\")\n",
        "    \n",
        "    return reloaded_count\n",
        "\n",
        "def setup_colab_sync():\n",
        "    \"\"\"Setup file synchronization for Colab\"\"\"\n",
        "    print(\"üîß Setting up file synchronization...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    if IN_COLAB:\n",
        "        print(\"üìã Running in Google Colab\")\n",
        "        print(\"üí° Local Watchdog Setup Instructions:\")\n",
        "        print(\"   1. On your local machine, install watchdog:\")\n",
        "        print(\"      pip install watchdog\")\n",
        "        print(\"   2. Run the sync script:\")\n",
        "        print(f\"      python local_watchdog_sync.py --local-path /path/to/your/project --project-name {os.path.basename(PROJECT_PATH)}\")\n",
        "        print(\"   3. Keep the script running while developing\")\n",
        "        print(\"   4. Your changes will automatically sync to Google Drive\")\n",
        "        print(\"   5. Use reload_modules() in this notebook to get the latest changes\")\n",
        "        \n",
        "        # Check current sync status\n",
        "        print(\"\\nüîç Checking sync status...\")\n",
        "        check_sync_status()\n",
        "        \n",
        "    else:\n",
        "        print(\"üìã Running locally - no sync needed\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Sync setup complete!\")\n",
        "\n",
        "# Setup sync system\n",
        "setup_colab_sync()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîÑ Module Reloading\n",
        "\n",
        "Use this cell to reload modules after making changes locally. Run this whenever you modify `.py` files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload modules to get latest changes from local development\n",
        "print(\"üîÑ Reloading modules...\")\n",
        "reloaded_count = reload_modules()\n",
        "\n",
        "if reloaded_count > 0:\n",
        "    print(f\"‚úÖ Successfully reloaded {reloaded_count} modules\")\n",
        "    print(\"üí° Your latest local changes are now available!\")\n",
        "else:\n",
        "    print(\"üìã No modules needed reloading\")\n",
        "\n",
        "# Check sync status\n",
        "print(\"\\nüîç Checking sync status...\")\n",
        "check_sync_status()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ‚öôÔ∏è Configuration Management\n",
        "\n",
        "Load and manage training configurations using the existing YAML configuration files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration management\n",
        "def list_available_configs():\n",
        "    \"\"\"List all available configuration files\"\"\"\n",
        "    config_dir = Path(PROJECT_PATH) / 'configs'\n",
        "    if not config_dir.exists():\n",
        "        print(\"‚ùå No configs directory found\")\n",
        "        return []\n",
        "    \n",
        "    configs = list(config_dir.glob('*.yaml'))\n",
        "    print(\"üìÅ Available configurations:\")\n",
        "    for i, config in enumerate(configs):\n",
        "        print(f\"  {i+1}. {config.name}\")\n",
        "    return configs\n",
        "\n",
        "def load_config(config_path):\n",
        "    \"\"\"Load configuration from YAML file\"\"\"\n",
        "    try:\n",
        "        with open(config_path, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "        print(f\"‚úÖ Configuration loaded from {config_path}\")\n",
        "        return config\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading config: {e}\")\n",
        "        return None\n",
        "\n",
        "def display_config(config):\n",
        "    \"\"\"Display configuration in a nice format\"\"\"\n",
        "    print(\"üìã Current Configuration:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for section, values in config.items():\n",
        "        print(f\"\\nüîß {section.upper()}:\")\n",
        "        if isinstance(values, dict):\n",
        "            for key, value in values.items():\n",
        "                if isinstance(value, dict):\n",
        "                    print(f\"  {key}:\")\n",
        "                    for subkey, subvalue in value.items():\n",
        "                        print(f\"    {subkey}: {subvalue}\")\n",
        "                else:\n",
        "                    print(f\"  {key}: {value}\")\n",
        "        else:\n",
        "            print(f\"  {values}\")\n",
        "\n",
        "def create_colab_config():\n",
        "    \"\"\"Create a Colab-optimized configuration\"\"\"\n",
        "    config = {\n",
        "        'data': {\n",
        "            'sequence_length': 200,\n",
        "            'channels': 1,\n",
        "            'train_split': 0.7,\n",
        "            'val_split': 0.15,\n",
        "            'test_split': 0.15,\n",
        "            'random_state': 42,\n",
        "            'augment_train': True,\n",
        "            'augment_prob': 0.5,\n",
        "            'noise_std': 0.01,\n",
        "            'time_shift_max': 50\n",
        "        },\n",
        "        'model': {\n",
        "            'type': 'original',  # or 'advanced'\n",
        "            'static_dim': 4,\n",
        "            'input_dim': 1,\n",
        "            'latent_dim': 128,\n",
        "            'condition_dim': 128,\n",
        "            'hidden_dim': 256,\n",
        "            'num_encoder_layers': 4,\n",
        "            'num_decoder_layers': 4,\n",
        "            'num_heads': 8,\n",
        "            'dropout': 0.1,\n",
        "            'sequence_length': 200,\n",
        "            'beta': 1.0\n",
        "        },\n",
        "        'training': {\n",
        "            'epochs': 50,  # Reduced for Colab\n",
        "            'batch_size': 16 if torch.cuda.is_available() else 8,  # Colab GPU friendly\n",
        "            'initial_beta': 0.0,\n",
        "            'final_beta': 1.0,\n",
        "            'beta_annealing_epochs': 25,\n",
        "            'early_stopping_patience': 10,\n",
        "            'checkpoint_interval': 5,\n",
        "            'sample_interval': 10,\n",
        "            'log_interval': 50,\n",
        "            'output_dir': 'outputs_colab',\n",
        "            'optimizer': {\n",
        "                'type': 'adamw',\n",
        "                'lr': 0.001,\n",
        "                'weight_decay': 0.0001,\n",
        "                'betas': [0.9, 0.999],\n",
        "                'eps': 1e-8\n",
        "            },\n",
        "            'scheduler': {\n",
        "                'enabled': True,\n",
        "                'type': 'cosine',\n",
        "                'min_lr': 1e-6\n",
        "            },\n",
        "            'gradient_clipping': {\n",
        "                'enabled': True,\n",
        "                'max_norm': 1.0\n",
        "            }\n",
        "        },\n",
        "        'evaluation': {\n",
        "            'num_reconstruction_samples': 500,\n",
        "            'num_generation_samples': 500,\n",
        "            'samples_per_condition': 3,\n",
        "            'num_interpolations': 5,\n",
        "            'num_latent_samples': 500\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return config\n",
        "\n",
        "# List available configurations\n",
        "available_configs = list_available_configs()\n",
        "\n",
        "# Load default configuration or create one for Colab\n",
        "if available_configs:\n",
        "    # Use the first config as default, or let user choose\n",
        "    default_config_path = available_configs[0]\n",
        "    print(f\"\\nüéØ Using default config: {default_config_path.name}\")\n",
        "    config = load_config(default_config_path)\n",
        "else:\n",
        "    print(\"\\nüéØ Creating Colab-optimized configuration\")\n",
        "    config = create_colab_config()\n",
        "\n",
        "if config:\n",
        "    display_config(config)\n",
        "else:\n",
        "    print(\"‚ùå Failed to load configuration\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üöÄ Training Pipeline\n",
        "\n",
        "Use the existing train.py module to train the CVAE model with real-time monitoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training setup and execution\n",
        "import subprocess\n",
        "import threading\n",
        "import queue\n",
        "import re\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class TrainingMonitor:\n",
        "    \"\"\"Monitor training progress in real-time\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.metrics = {\n",
        "            'epochs': [],\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'kl_loss': [],\n",
        "            'recon_loss': [],\n",
        "            'beta': []\n",
        "        }\n",
        "        self.current_epoch = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "        \n",
        "    def parse_log_line(self, line):\n",
        "        \"\"\"Parse training log line and extract metrics\"\"\"\n",
        "        try:\n",
        "            # Extract epoch info\n",
        "            epoch_patterns = [\n",
        "                r'Epoch (\\d+)/(\\d+)',\n",
        "                r'Epoch (\\d+):',\n",
        "                r'Starting epoch (\\d+)'\n",
        "            ]\n",
        "            for pattern in epoch_patterns:\n",
        "                epoch_match = re.search(pattern, line)\n",
        "                if epoch_match:\n",
        "                    self.current_epoch = int(epoch_match.group(1))\n",
        "                    break\n",
        "                \n",
        "            # Extract losses with multiple patterns\n",
        "            train_loss_patterns = [\n",
        "                r'Train Loss: ([\\d.]+)',\n",
        "                r'Training Loss: ([\\d.]+)',\n",
        "                r'train_loss: ([\\d.]+)'\n",
        "            ]\n",
        "            for pattern in train_loss_patterns:\n",
        "                train_loss_match = re.search(pattern, line)\n",
        "                if train_loss_match:\n",
        "                    self.metrics['train_loss'].append(float(train_loss_match.group(1)))\n",
        "                    break\n",
        "                \n",
        "            val_loss_patterns = [\n",
        "                r'Val Loss: ([\\d.]+)',\n",
        "                r'Validation Loss: ([\\d.]+)',\n",
        "                r'val_loss: ([\\d.]+)'\n",
        "            ]\n",
        "            for pattern in val_loss_patterns:\n",
        "                val_loss_match = re.search(pattern, line)\n",
        "                if val_loss_match:\n",
        "                    val_loss = float(val_loss_match.group(1))\n",
        "                    self.metrics['val_loss'].append(val_loss)\n",
        "                    if val_loss < self.best_val_loss:\n",
        "                        self.best_val_loss = val_loss\n",
        "                    break\n",
        "                    \n",
        "            # Extract KL loss\n",
        "            kl_patterns = [\n",
        "                r'KL Loss: ([\\d.]+)',\n",
        "                r'KL: ([\\d.]+)',\n",
        "                r'kl_loss: ([\\d.]+)'\n",
        "            ]\n",
        "            for pattern in kl_patterns:\n",
        "                kl_loss_match = re.search(pattern, line)\n",
        "                if kl_loss_match:\n",
        "                    self.metrics['kl_loss'].append(float(kl_loss_match.group(1)))\n",
        "                    break\n",
        "                    \n",
        "            # Extract reconstruction loss\n",
        "            recon_patterns = [\n",
        "                r'Recon Loss: ([\\d.]+)',\n",
        "                r'Reconstruction Loss: ([\\d.]+)',\n",
        "                r'recon_loss: ([\\d.]+)',\n",
        "                r'Recon: ([\\d.]+)'\n",
        "            ]\n",
        "            for pattern in recon_patterns:\n",
        "                recon_loss_match = re.search(pattern, line)\n",
        "                if recon_loss_match:\n",
        "                    self.metrics['recon_loss'].append(float(recon_loss_match.group(1)))\n",
        "                    break\n",
        "                    \n",
        "            # Extract beta\n",
        "            beta_patterns = [\n",
        "                r'Beta: ([\\d.]+)',\n",
        "                r'beta: ([\\d.]+)'\n",
        "            ]\n",
        "            for pattern in beta_patterns:\n",
        "                beta_match = re.search(pattern, line)\n",
        "                if beta_match:\n",
        "                    self.metrics['beta'].append(float(beta_match.group(1)))\n",
        "                    break\n",
        "                    \n",
        "        except Exception as e:\n",
        "            # Silently continue if parsing fails\n",
        "            pass\n",
        "    \n",
        "    def plot_progress(self):\n",
        "        \"\"\"Plot training progress\"\"\"\n",
        "        if not self.metrics['train_loss']:\n",
        "            return\n",
        "            \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle(f'Training Progress - Epoch {self.current_epoch}', fontsize=16)\n",
        "        \n",
        "        # Loss curves\n",
        "        if self.metrics['train_loss'] and self.metrics['val_loss']:\n",
        "            epochs = range(1, len(self.metrics['train_loss']) + 1)\n",
        "            axes[0, 0].plot(epochs, self.metrics['train_loss'], label='Train Loss', color='blue')\n",
        "            axes[0, 0].plot(epochs, self.metrics['val_loss'], label='Val Loss', color='red')\n",
        "            axes[0, 0].set_title('Training & Validation Loss')\n",
        "            axes[0, 0].set_xlabel('Epoch')\n",
        "            axes[0, 0].set_ylabel('Loss')\n",
        "            axes[0, 0].legend()\n",
        "            axes[0, 0].grid(True)\n",
        "        \n",
        "        # KL and Reconstruction Loss\n",
        "        if self.metrics['kl_loss'] and self.metrics['recon_loss']:\n",
        "            epochs = range(1, len(self.metrics['kl_loss']) + 1)\n",
        "            axes[0, 1].plot(epochs, self.metrics['kl_loss'], label='KL Loss', color='green')\n",
        "            axes[0, 1].plot(epochs, self.metrics['recon_loss'], label='Recon Loss', color='orange')\n",
        "            axes[0, 1].set_title('KL & Reconstruction Loss')\n",
        "            axes[0, 1].set_xlabel('Epoch')\n",
        "            axes[0, 1].set_ylabel('Loss')\n",
        "            axes[0, 1].legend()\n",
        "            axes[0, 1].grid(True)\n",
        "        \n",
        "        # Beta annealing\n",
        "        if self.metrics['beta']:\n",
        "            epochs = range(1, len(self.metrics['beta']) + 1)\n",
        "            axes[1, 0].plot(epochs, self.metrics['beta'], label='Beta', color='purple')\n",
        "            axes[1, 0].set_title('Beta Annealing')\n",
        "            axes[1, 0].set_xlabel('Epoch')\n",
        "            axes[1, 0].set_ylabel('Beta Value')\n",
        "            axes[1, 0].legend()\n",
        "            axes[1, 0].grid(True)\n",
        "        \n",
        "        # Summary stats\n",
        "        axes[1, 1].axis('off')\n",
        "        \n",
        "        # Format stats with proper conditional handling\n",
        "        latest_train = f\"{self.metrics['train_loss'][-1]:.4f}\" if self.metrics['train_loss'] else 'N/A'\n",
        "        latest_val = f\"{self.metrics['val_loss'][-1]:.4f}\" if self.metrics['val_loss'] else 'N/A'\n",
        "        current_beta = f\"{self.metrics['beta'][-1]:.4f}\" if self.metrics['beta'] else 'N/A'\n",
        "        \n",
        "        stats_text = f\"\"\"\n",
        "        Current Epoch: {self.current_epoch}\n",
        "        Best Val Loss: {self.best_val_loss:.4f}\n",
        "        Latest Train Loss: {latest_train}\n",
        "        Latest Val Loss: {latest_val}\n",
        "        Current Beta: {current_beta}\n",
        "        \"\"\"\n",
        "        axes[1, 1].text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def print_summary(self):\n",
        "        \"\"\"Print a simple text summary of training progress\"\"\"\n",
        "        print(f\"üìä Training Summary:\")\n",
        "        print(f\"   Current Epoch: {self.current_epoch}\")\n",
        "        print(f\"   Best Val Loss: {self.best_val_loss:.4f}\")\n",
        "        if self.metrics['train_loss']:\n",
        "            print(f\"   Latest Train Loss: {self.metrics['train_loss'][-1]:.4f}\")\n",
        "        if self.metrics['val_loss']:\n",
        "            print(f\"   Latest Val Loss: {self.metrics['val_loss'][-1]:.4f}\")\n",
        "        if self.metrics['beta']:\n",
        "            print(f\"   Current Beta: {self.metrics['beta'][-1]:.4f}\")\n",
        "        print(f\"   Total Epochs Logged: {len(self.metrics['train_loss'])}\")\n",
        "\n",
        "def run_training_with_monitoring(config, output_dir=\"outputs_colab\"):\n",
        "    \"\"\"Run training with real-time monitoring\"\"\"\n",
        "    \n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Save config for training\n",
        "    config_path = os.path.join(output_dir, \"colab_config.yaml\")\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(config, f, default_flow_style=False)\n",
        "    \n",
        "    # Initialize monitor\n",
        "    monitor = TrainingMonitor()\n",
        "    \n",
        "    # Prepare training command\n",
        "    cmd = [\n",
        "        sys.executable, \"train.py\",\n",
        "        \"--config\", config_path,\n",
        "        \"--output-dir\", output_dir,\n",
        "        \"--device\", str(device)\n",
        "    ]\n",
        "    \n",
        "    print(f\"üöÄ Starting training with command: {' '.join(cmd)}\")\n",
        "    print(\"üìä Training progress will be displayed below...\")\n",
        "    \n",
        "    # Run training process\n",
        "    try:\n",
        "        process = subprocess.Popen(\n",
        "            cmd,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            universal_newlines=True,\n",
        "            bufsize=1\n",
        "        )\n",
        "        \n",
        "        # Monitor output in real-time\n",
        "        line_count = 0\n",
        "        last_plot_update = 0\n",
        "        \n",
        "        for line in iter(process.stdout.readline, ''):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                line_count += 1\n",
        "                \n",
        "                # Only print important lines to avoid spam\n",
        "                if any(keyword in line for keyword in [\n",
        "                    'Epoch', 'Loss:', 'Starting', 'Model', 'Training', 'Validation', \n",
        "                    'Best', 'Saved', 'Early stopping', 'completed', 'ERROR', 'WARNING'\n",
        "                ]):\n",
        "                    print(line)\n",
        "                \n",
        "                # Parse metrics from all lines\n",
        "                monitor.parse_log_line(line)\n",
        "                \n",
        "                # Update plots less frequently to avoid spam\n",
        "                if ('Validation Loss:' in line or 'Val Loss:' in line) and (line_count - last_plot_update > 50):\n",
        "                    last_plot_update = line_count\n",
        "                    clear_output(wait=True)\n",
        "                    print(\"üîÑ Updating training progress...\")\n",
        "                    monitor.print_summary()\n",
        "                    monitor.plot_progress()\n",
        "                \n",
        "                # Show progress every 200 lines for very verbose output  \n",
        "                elif line_count % 200 == 0:\n",
        "                    print(f\"üìä Training in progress... ({line_count} lines processed)\")\n",
        "                    if monitor.current_epoch > 0:\n",
        "                        monitor.print_summary()\n",
        "        \n",
        "        # Final plot update\n",
        "        if monitor.metrics['train_loss']:\n",
        "            clear_output(wait=True)\n",
        "            print(\"üéØ Training completed! Final results:\")\n",
        "            monitor.plot_progress()\n",
        "        \n",
        "        # Wait for process to complete\n",
        "        return_code = process.wait()\n",
        "        \n",
        "        if return_code == 0:\n",
        "            print(\"‚úÖ Training completed successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå Training failed with return code: {return_code}\")\n",
        "            return False\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during training: {e}\")\n",
        "        return False\n",
        "\n",
        "# Training execution\n",
        "def start_training():\n",
        "    \"\"\"Start the training process\"\"\"\n",
        "    if not config:\n",
        "        print(\"‚ùå No configuration loaded. Please run the configuration cell first.\")\n",
        "        return False\n",
        "    \n",
        "    print(\"üéØ Starting CVAE training...\")\n",
        "    print(f\"üìã Model type: {config['model'].get('type', 'original')}\")\n",
        "    print(f\"üìã Epochs: {config['training']['epochs']}\")\n",
        "    print(f\"üìã Batch size: {config['training']['batch_size']}\")\n",
        "    print(f\"üìã Device: {device}\")\n",
        "    \n",
        "    # Ensure data directory exists\n",
        "    data_dir = os.path.join(PROJECT_PATH, \"data\", \"processed\")\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"‚ùå Data directory not found: {data_dir}\")\n",
        "        print(\"üìã Please ensure your preprocessed data is available.\")\n",
        "        return False\n",
        "    \n",
        "    return run_training_with_monitoring(config)\n",
        "\n",
        "print(\"‚úÖ Training pipeline ready!\")\n",
        "print(\"üìã Run start_training() to begin training\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üß™ Evaluation Pipeline\n",
        "\n",
        "Use the existing evaluate.py module to comprehensively evaluate the trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation setup and execution\n",
        "def find_best_checkpoint(output_dir=\"outputs_colab\"):\n",
        "    \"\"\"Find the best checkpoint from training\"\"\"\n",
        "    output_path = Path(output_dir)\n",
        "    \n",
        "    if not output_path.exists():\n",
        "        print(f\"‚ùå Output directory not found: {output_dir}\")\n",
        "        return None\n",
        "    \n",
        "    # Look for best checkpoint\n",
        "    best_checkpoint = output_path / \"best_checkpoint.pth\"\n",
        "    if best_checkpoint.exists():\n",
        "        print(f\"‚úÖ Found best checkpoint: {best_checkpoint}\")\n",
        "        return str(best_checkpoint)\n",
        "    \n",
        "    # Look for any checkpoint files\n",
        "    checkpoints = list(output_path.glob(\"checkpoint_epoch_*.pth\"))\n",
        "    if checkpoints:\n",
        "        # Return the latest checkpoint\n",
        "        latest_checkpoint = max(checkpoints, key=lambda p: p.stat().st_mtime)\n",
        "        print(f\"‚úÖ Found latest checkpoint: {latest_checkpoint}\")\n",
        "        return str(latest_checkpoint)\n",
        "    \n",
        "    print(\"‚ùå No checkpoint files found\")\n",
        "    return None\n",
        "\n",
        "def run_evaluation(checkpoint_path, output_dir=\"results_colab\"):\n",
        "    \"\"\"Run comprehensive evaluation using evaluate.py\"\"\"\n",
        "    \n",
        "    if not checkpoint_path or not os.path.exists(checkpoint_path):\n",
        "        print(f\"‚ùå Checkpoint not found: {checkpoint_path}\")\n",
        "        return False\n",
        "    \n",
        "    # Create results directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Prepare evaluation command\n",
        "    cmd = [\n",
        "        sys.executable, \"evaluate.py\",\n",
        "        \"--model\", checkpoint_path,\n",
        "        \"--output-dir\", output_dir,\n",
        "        \"--comprehensive\"\n",
        "    ]\n",
        "    \n",
        "    print(f\"üî¨ Starting evaluation with command: {' '.join(cmd)}\")\n",
        "    print(\"üìä Evaluation progress will be displayed below...\")\n",
        "    \n",
        "    try:\n",
        "        # Run evaluation process\n",
        "        result = subprocess.run(\n",
        "            cmd,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=3600  # 1 hour timeout\n",
        "        )\n",
        "        \n",
        "        # Display output\n",
        "        if result.stdout:\n",
        "            print(\"üìã Evaluation Output:\")\n",
        "            print(result.stdout)\n",
        "        \n",
        "        if result.stderr:\n",
        "            print(\"‚ö†Ô∏è Evaluation Warnings/Errors:\")\n",
        "            print(result.stderr)\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Evaluation completed successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå Evaluation failed with return code: {result.returncode}\")\n",
        "            return False\n",
        "            \n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"‚ùå Evaluation timed out after 1 hour\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during evaluation: {e}\")\n",
        "        return False\n",
        "\n",
        "def display_evaluation_results(results_dir=\"results_colab\"):\n",
        "    \"\"\"Display evaluation results\"\"\"\n",
        "    results_path = Path(results_dir)\n",
        "    \n",
        "    if not results_path.exists():\n",
        "        print(f\"‚ùå Results directory not found: {results_dir}\")\n",
        "        return\n",
        "    \n",
        "    print(\"üìä Evaluation Results:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Look for evaluation subdirectories\n",
        "    eval_dirs = [d for d in results_path.iterdir() if d.is_dir() and d.name.startswith('eval_')]\n",
        "    \n",
        "    if not eval_dirs:\n",
        "        print(\"‚ùå No evaluation results found\")\n",
        "        return\n",
        "    \n",
        "    # Use the most recent evaluation\n",
        "    latest_eval_dir = max(eval_dirs, key=lambda d: d.stat().st_mtime)\n",
        "    print(f\"üìÅ Latest evaluation: {latest_eval_dir.name}\")\n",
        "    \n",
        "    # Display metadata\n",
        "    metadata_file = latest_eval_dir / \"evaluation_metadata.json\"\n",
        "    if metadata_file.exists():\n",
        "        with open(metadata_file, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "        print(\"\\\\nüìã Evaluation Metadata:\")\n",
        "        for key, value in metadata.items():\n",
        "            if isinstance(value, dict):\n",
        "                print(f\"  {key}:\")\n",
        "                for subkey, subvalue in value.items():\n",
        "                    print(f\"    {subkey}: {subvalue}\")\n",
        "            else:\n",
        "                print(f\"  {key}: {value}\")\n",
        "    \n",
        "    # Look for generated plots\n",
        "    plot_files = list(latest_eval_dir.glob(\"*.png\"))\n",
        "    if plot_files:\n",
        "        print(f\"\\\\nüñºÔ∏è Generated plots ({len(plot_files)} files):\")\n",
        "        for plot_file in plot_files[:5]:  # Show first 5\n",
        "            print(f\"  üìä {plot_file.name}\")\n",
        "    \n",
        "    # Look for CSV results\n",
        "    csv_files = list(latest_eval_dir.glob(\"*.csv\"))\n",
        "    if csv_files:\n",
        "        print(f\"\\\\nüìà CSV results ({len(csv_files)} files):\")\n",
        "        for csv_file in csv_files:\n",
        "            print(f\"  üìÑ {csv_file.name}\")\n",
        "            \n",
        "            # Display first few rows if it's a small file\n",
        "            try:\n",
        "                df = pd.read_csv(csv_file)\n",
        "                if len(df) <= 10:\n",
        "                    print(f\"    Preview:\")\n",
        "                    print(df.to_string(index=False, max_rows=5))\n",
        "                else:\n",
        "                    print(f\"    Shape: {df.shape}\")\n",
        "                    print(f\"    Columns: {list(df.columns)}\")\n",
        "            except Exception as e:\n",
        "                print(f\"    Error reading CSV: {e}\")\n",
        "\n",
        "def start_evaluation():\n",
        "    \"\"\"Start the evaluation process\"\"\"\n",
        "    \n",
        "    # Find the best checkpoint\n",
        "    checkpoint_path = find_best_checkpoint()\n",
        "    \n",
        "    if not checkpoint_path:\n",
        "        print(\"‚ùå No checkpoint found. Please ensure training has completed.\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"üéØ Starting evaluation of checkpoint: {checkpoint_path}\")\n",
        "    \n",
        "    # Run evaluation\n",
        "    success = run_evaluation(checkpoint_path)\n",
        "    \n",
        "    if success:\n",
        "        print(\"\\\\nüìä Displaying evaluation results...\")\n",
        "        display_evaluation_results()\n",
        "    \n",
        "    return success\n",
        "\n",
        "print(\"‚úÖ Evaluation pipeline ready!\")\n",
        "print(\"üìã Run start_evaluation() to evaluate the trained model\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä Quick Training Execution\n",
        "\n",
        "Run this cell to start training immediately with default settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick training execution\n",
        "print(\"üöÄ Starting training with current configuration...\")\n",
        "training_success = start_training()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üß™ Quick Evaluation Execution\n",
        "\n",
        "Run this cell to evaluate the trained model immediately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick evaluation execution\n",
        "print(\"üî¨ Starting evaluation of trained model...\")\n",
        "evaluation_success = start_evaluation()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß Advanced Configuration\n",
        "\n",
        "Customize training parameters and model architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced configuration options\n",
        "def create_custom_config():\n",
        "    \"\"\"Create custom configuration with advanced options\"\"\"\n",
        "    \n",
        "    print(\"üîß Custom Configuration Builder\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Model type selection\n",
        "    print(\"\\\\n1. Model Type:\")\n",
        "    print(\"   a) Original CVAE\")\n",
        "    print(\"   b) Advanced CVAE (hierarchical latents)\")\n",
        "    \n",
        "    model_type = input(\"Choose model type (a/b): \").lower()\n",
        "    if model_type == 'b':\n",
        "        model_type = 'advanced'\n",
        "    else:\n",
        "        model_type = 'original'\n",
        "    \n",
        "    # Training parameters\n",
        "    print(\"\\\\n2. Training Parameters:\")\n",
        "    epochs = int(input(\"Number of epochs (default 50): \") or \"50\")\n",
        "    batch_size = int(input(\"Batch size (default 16): \") or \"16\")\n",
        "    learning_rate = float(input(\"Learning rate (default 0.001): \") or \"0.001\")\n",
        "    \n",
        "    # Model architecture\n",
        "    print(\"\\\\n3. Model Architecture:\")\n",
        "    latent_dim = int(input(\"Latent dimension (default 128): \") or \"128\")\n",
        "    hidden_dim = int(input(\"Hidden dimension (default 256): \") or \"256\")\n",
        "    \n",
        "    # Create custom config\n",
        "    custom_config = {\n",
        "        'data': {\n",
        "            'sequence_length': 200,\n",
        "            'channels': 1,\n",
        "            'train_split': 0.7,\n",
        "            'val_split': 0.15,\n",
        "            'test_split': 0.15,\n",
        "            'random_state': 42,\n",
        "            'augment_train': True,\n",
        "            'augment_prob': 0.5,\n",
        "            'noise_std': 0.01,\n",
        "            'time_shift_max': 50\n",
        "        },\n",
        "        'model': {\n",
        "            'type': model_type,\n",
        "            'static_dim': 4,\n",
        "            'input_dim': 1,\n",
        "            'latent_dim': latent_dim,\n",
        "            'condition_dim': 128,\n",
        "            'hidden_dim': hidden_dim,\n",
        "            'num_encoder_layers': 4,\n",
        "            'num_decoder_layers': 4,\n",
        "            'num_heads': 8,\n",
        "            'dropout': 0.1,\n",
        "            'sequence_length': 200,\n",
        "            'beta': 1.0\n",
        "        },\n",
        "        'training': {\n",
        "            'epochs': epochs,\n",
        "            'batch_size': batch_size,\n",
        "            'initial_beta': 0.0,\n",
        "            'final_beta': 1.0,\n",
        "            'beta_annealing_epochs': epochs // 2,\n",
        "            'early_stopping_patience': max(10, epochs // 5),\n",
        "            'checkpoint_interval': max(5, epochs // 10),\n",
        "            'sample_interval': max(10, epochs // 5),\n",
        "            'log_interval': 50,\n",
        "            'output_dir': 'outputs_custom',\n",
        "            'optimizer': {\n",
        "                'type': 'adamw',\n",
        "                'lr': learning_rate,\n",
        "                'weight_decay': 0.0001,\n",
        "                'betas': [0.9, 0.999],\n",
        "                'eps': 1e-8\n",
        "            },\n",
        "            'scheduler': {\n",
        "                'enabled': True,\n",
        "                'type': 'cosine',\n",
        "                'min_lr': 1e-6\n",
        "            },\n",
        "            'gradient_clipping': {\n",
        "                'enabled': True,\n",
        "                'max_norm': 1.0\n",
        "            }\n",
        "        },\n",
        "        'evaluation': {\n",
        "            'num_reconstruction_samples': 500,\n",
        "            'num_generation_samples': 500,\n",
        "            'samples_per_condition': 3,\n",
        "            'num_interpolations': 5,\n",
        "            'num_latent_samples': 500\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(\"\\\\n‚úÖ Custom configuration created!\")\n",
        "    return custom_config\n",
        "\n",
        "def compare_models():\n",
        "    \"\"\"Compare different model configurations\"\"\"\n",
        "    \n",
        "    # Original model config\n",
        "    original_config = create_colab_config()\n",
        "    original_config['model']['type'] = 'original'\n",
        "    original_config['training']['output_dir'] = 'outputs_original'\n",
        "    \n",
        "    # Advanced model config\n",
        "    advanced_config = create_colab_config()\n",
        "    advanced_config['model']['type'] = 'advanced'\n",
        "    advanced_config['training']['output_dir'] = 'outputs_advanced'\n",
        "    \n",
        "    configs = {\n",
        "        'Original CVAE': original_config,\n",
        "        'Advanced CVAE': advanced_config\n",
        "    }\n",
        "    \n",
        "    print(\"üî¨ Model Comparison Mode\")\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for model_name, model_config in configs.items():\n",
        "        print(f\"\\\\nüöÄ Training {model_name}...\")\n",
        "        \n",
        "        # Update global config\n",
        "        global config\n",
        "        config = model_config\n",
        "        \n",
        "        # Train model\n",
        "        success = run_training_with_monitoring(model_config, model_config['training']['output_dir'])\n",
        "        \n",
        "        if success:\n",
        "            print(f\"‚úÖ {model_name} training completed!\")\n",
        "            \n",
        "            # Evaluate model\n",
        "            checkpoint_path = find_best_checkpoint(model_config['training']['output_dir'])\n",
        "            if checkpoint_path:\n",
        "                eval_success = run_evaluation(checkpoint_path, f\"results_{model_name.lower().replace(' ', '_')}\")\n",
        "                results[model_name] = {\n",
        "                    'training_success': True,\n",
        "                    'evaluation_success': eval_success,\n",
        "                    'checkpoint_path': checkpoint_path\n",
        "                }\n",
        "            else:\n",
        "                results[model_name] = {\n",
        "                    'training_success': True,\n",
        "                    'evaluation_success': False,\n",
        "                    'checkpoint_path': None\n",
        "                }\n",
        "        else:\n",
        "            print(f\"‚ùå {model_name} training failed!\")\n",
        "            results[model_name] = {\n",
        "                'training_success': False,\n",
        "                'evaluation_success': False,\n",
        "                'checkpoint_path': None\n",
        "            }\n",
        "    \n",
        "    # Display comparison results\n",
        "    print(\"\\\\nüìä Model Comparison Results:\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    for model_name, result in results.items():\n",
        "        print(f\"\\\\nüî¨ {model_name}:\")\n",
        "        print(f\"  Training: {'‚úÖ' if result['training_success'] else '‚ùå'}\")\n",
        "        print(f\"  Evaluation: {'‚úÖ' if result['evaluation_success'] else '‚ùå'}\")\n",
        "        if result['checkpoint_path']:\n",
        "            print(f\"  Checkpoint: {result['checkpoint_path']}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Advanced configuration tools ready!\")\n",
        "print(\"üìã Available functions:\")\n",
        "print(\"  - create_custom_config(): Build custom configuration\")\n",
        "print(\"  - compare_models(): Compare original vs advanced models\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üíæ Google Drive Integration\n",
        "\n",
        "Save results and checkpoints to Google Drive for persistence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Drive integration\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "def backup_to_drive():\n",
        "    \"\"\"Backup training results to Google Drive\"\"\"\n",
        "    \n",
        "    if not IN_COLAB:\n",
        "        print(\"üìã Drive backup is only available in Google Colab\")\n",
        "        return\n",
        "    \n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    backup_dir = f\"/content/drive/MyDrive/abr_cvae_backups/backup_{timestamp}\"\n",
        "    \n",
        "    print(f\"üíæ Creating backup in Google Drive: {backup_dir}\")\n",
        "    \n",
        "    try:\n",
        "        os.makedirs(backup_dir, exist_ok=True)\n",
        "        \n",
        "        # Backup outputs\n",
        "        output_dirs = [\"outputs_colab\", \"outputs_original\", \"outputs_advanced\", \"outputs_custom\"]\n",
        "        for output_dir in output_dirs:\n",
        "            if os.path.exists(output_dir):\n",
        "                print(f\"üìÅ Backing up {output_dir}...\")\n",
        "                shutil.copytree(output_dir, os.path.join(backup_dir, output_dir))\n",
        "        \n",
        "        # Backup results\n",
        "        result_dirs = [\"results_colab\", \"results_original_cvae\", \"results_advanced_cvae\"]\n",
        "        for result_dir in result_dirs:\n",
        "            if os.path.exists(result_dir):\n",
        "                print(f\"üìä Backing up {result_dir}...\")\n",
        "                shutil.copytree(result_dir, os.path.join(backup_dir, result_dir))\n",
        "        \n",
        "        # Create backup metadata\n",
        "        metadata = {\n",
        "            \"backup_timestamp\": timestamp,\n",
        "            \"project_path\": PROJECT_PATH,\n",
        "            \"device_used\": str(device),\n",
        "            \"torch_version\": torch.__version__,\n",
        "            \"python_version\": sys.version\n",
        "        }\n",
        "        \n",
        "        metadata_path = os.path.join(backup_dir, \"backup_metadata.json\")\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        \n",
        "        print(f\"‚úÖ Backup completed successfully!\")\n",
        "        print(f\"üìÅ Backup location: {backup_dir}\")\n",
        "        \n",
        "        return backup_dir\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Backup failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def restore_from_drive():\n",
        "    \"\"\"Restore training results from Google Drive\"\"\"\n",
        "    \n",
        "    if not IN_COLAB:\n",
        "        print(\"üìã Drive restore is only available in Google Colab\")\n",
        "        return\n",
        "    \n",
        "    backup_base = \"/content/drive/MyDrive/abr_cvae_backups\"\n",
        "    \n",
        "    if not os.path.exists(backup_base):\n",
        "        print(\"‚ùå No backups found in Google Drive\")\n",
        "        return\n",
        "    \n",
        "    # List available backups\n",
        "    backups = [d for d in os.listdir(backup_base) if d.startswith(\"backup_\")]\n",
        "    backups.sort(reverse=True)  # Most recent first\n",
        "    \n",
        "    if not backups:\n",
        "        print(\"‚ùå No backups found\")\n",
        "        return\n",
        "    \n",
        "    print(\"üìÅ Available backups:\")\n",
        "    for i, backup in enumerate(backups[:5]):  # Show last 5 backups\n",
        "        print(f\"  {i+1}. {backup}\")\n",
        "    \n",
        "    try:\n",
        "        choice = int(input(\"Choose backup to restore (1-5): \")) - 1\n",
        "        if 0 <= choice < len(backups):\n",
        "            backup_path = os.path.join(backup_base, backups[choice])\n",
        "            \n",
        "            print(f\"üîÑ Restoring from: {backup_path}\")\n",
        "            \n",
        "            # Restore directories\n",
        "            for item in os.listdir(backup_path):\n",
        "                item_path = os.path.join(backup_path, item)\n",
        "                if os.path.isdir(item_path) and not item.endswith('.json'):\n",
        "                    print(f\"üìÅ Restoring {item}...\")\n",
        "                    if os.path.exists(item):\n",
        "                        shutil.rmtree(item)\n",
        "                    shutil.copytree(item_path, item)\n",
        "            \n",
        "            print(\"‚úÖ Restore completed successfully!\")\n",
        "            \n",
        "        else:\n",
        "            print(\"‚ùå Invalid choice\")\n",
        "            \n",
        "    except ValueError:\n",
        "        print(\"‚ùå Invalid input\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Restore failed: {e}\")\n",
        "\n",
        "def sync_to_drive(source_dir, drive_subdir=\"abr_cvae_sync\"):\n",
        "    \"\"\"Sync specific directory to Google Drive\"\"\"\n",
        "    \n",
        "    if not IN_COLAB:\n",
        "        print(\"üìã Drive sync is only available in Google Colab\")\n",
        "        return\n",
        "    \n",
        "    if not os.path.exists(source_dir):\n",
        "        print(f\"‚ùå Source directory not found: {source_dir}\")\n",
        "        return\n",
        "    \n",
        "    drive_path = f\"/content/drive/MyDrive/{drive_subdir}\"\n",
        "    sync_path = os.path.join(drive_path, os.path.basename(source_dir))\n",
        "    \n",
        "    try:\n",
        "        os.makedirs(drive_path, exist_ok=True)\n",
        "        \n",
        "        print(f\"üîÑ Syncing {source_dir} to {sync_path}...\")\n",
        "        \n",
        "        if os.path.exists(sync_path):\n",
        "            shutil.rmtree(sync_path)\n",
        "        \n",
        "        shutil.copytree(source_dir, sync_path)\n",
        "        \n",
        "        print(f\"‚úÖ Sync completed: {sync_path}\")\n",
        "        return sync_path\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Sync failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Auto-backup function\n",
        "def setup_auto_backup():\n",
        "    \"\"\"Setup automatic backup every hour\"\"\"\n",
        "    \n",
        "    if not IN_COLAB:\n",
        "        print(\"üìã Auto-backup is only available in Google Colab\")\n",
        "        return\n",
        "    \n",
        "    def backup_worker():\n",
        "        while True:\n",
        "            time.sleep(3600)  # Wait 1 hour\n",
        "            print(\"‚è∞ Performing automatic backup...\")\n",
        "            backup_to_drive()\n",
        "    \n",
        "    backup_thread = threading.Thread(target=backup_worker, daemon=True)\n",
        "    backup_thread.start()\n",
        "    \n",
        "    print(\"‚è∞ Auto-backup enabled (every hour)\")\n",
        "\n",
        "print(\"‚úÖ Google Drive integration ready!\")\n",
        "print(\"üìã Available functions:\")\n",
        "print(\"  - backup_to_drive(): Backup all results to Google Drive\")\n",
        "print(\"  - restore_from_drive(): Restore from previous backup\")\n",
        "print(\"  - sync_to_drive(dir): Sync specific directory to Drive\")\n",
        "print(\"  - setup_auto_backup(): Enable automatic hourly backups\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Summary and Next Steps\n",
        "\n",
        "This notebook provides a complete pipeline for training and evaluating ABR CVAE models in Google Colab with watchdog integration.\n",
        "\n",
        "### Key Features:\n",
        "1. **Watchdog Integration**: Automatically syncs file changes from your local environment\n",
        "2. **Modular Design**: Uses existing `.py` files without modification\n",
        "3. **Real-time Monitoring**: Training progress visualization\n",
        "4. **Comprehensive Evaluation**: Complete model assessment\n",
        "5. **Google Drive Integration**: Automatic backup and sync\n",
        "6. **Model Comparison**: Compare different architectures\n",
        "\n",
        "### Workflow:\n",
        "1. **Setup**: Run setup cells to configure environment and watchdog\n",
        "2. **Configuration**: Load or create training configuration\n",
        "3. **Training**: Execute training with real-time monitoring\n",
        "4. **Evaluation**: Comprehensive model evaluation\n",
        "5. **Backup**: Save results to Google Drive\n",
        "\n",
        "### Advanced Features:\n",
        "- Custom configuration builder\n",
        "- Model comparison utilities\n",
        "- Automatic backup system\n",
        "- File synchronization with Drive\n",
        "\n",
        "### Tips for Using with Watchdog:\n",
        "1. **Local Development**: Make changes to your `.py` files locally\n",
        "2. **Automatic Sync**: Watchdog will detect changes and reload modules\n",
        "3. **Drive Sync**: Ensure your project is synced to Google Drive\n",
        "4. **Backup Regularly**: Use the backup functions to save progress\n",
        "\n",
        "### Troubleshooting:\n",
        "- Ensure all `.py` files are in the correct directory structure\n",
        "- Check that preprocessed data is available in `data/processed/`\n",
        "- Monitor GPU memory usage in Colab\n",
        "- Use the backup functions to save work periodically\n",
        "\n",
        "**Ready to start training! üöÄ**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "abr_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
