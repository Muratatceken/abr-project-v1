{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üß† ABR CVAE Training & Evaluation - Google Colab\n",
        "\n",
        "**Complete training and evaluation environment for ABR CVAE project in Google Colab**\n",
        "\n",
        "## üéØ Features:\n",
        "- **Uses Existing Project**: Works with your current train.py, evaluate.py, and src/ files\n",
        "- **Google Drive Storage**: Saves results to `/content/drive/MyDrive/abr_project/`\n",
        "- **Real-time Monitoring**: Live training progress visualization\n",
        "- **No New Files**: Uses only your existing project structure\n",
        "\n",
        "## üìã Setup Instructions:\n",
        "1. Upload your entire project to Google Drive at `/MyDrive/abr_project/`\n",
        "2. Run all cells in order\n",
        "3. Use the quick functions at the bottom to train and evaluate\n",
        "\n",
        "---\n",
        "*Murat At√ßeken - ABR CVAE Project*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Environment Setup\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"üîç Running in Google Colab\")\n",
        "    \n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Project paths\n",
        "    DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/abr_project\"\n",
        "    LOCAL_PROJECT_PATH = \"/content/abr_project\"\n",
        "    \n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"üîç Running locally\")\n",
        "    LOCAL_PROJECT_PATH = os.getcwd()\n",
        "    DRIVE_PROJECT_PATH = None\n",
        "\n",
        "print(f\"‚úÖ Environment detected: {'Colab' if IN_COLAB else 'Local'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìÅ Project Setup - Copy from Drive to Local\n",
        "if IN_COLAB:\n",
        "    if os.path.exists(f\"{DRIVE_PROJECT_PATH}/train.py\"):\n",
        "        print(\"üìÅ Project found in Google Drive, copying to local...\")\n",
        "        !cp -r \"{DRIVE_PROJECT_PATH}\" \"{LOCAL_PROJECT_PATH}\"\n",
        "        print(f\"‚úÖ Project copied to: {LOCAL_PROJECT_PATH}\")\n",
        "    else:\n",
        "        print(\"‚ùå Project not found in Google Drive!\")\n",
        "        print(f\"üí° Please upload your project to: {DRIVE_PROJECT_PATH}\")\n",
        "        print(\"üìã Required files: train.py, evaluate.py, src/, configs/, data/\")\n",
        "        raise FileNotFoundError(\"Project files not found in Google Drive\")\n",
        "    \n",
        "    # Set working directory\n",
        "    os.chdir(LOCAL_PROJECT_PATH)\n",
        "    sys.path.insert(0, LOCAL_PROJECT_PATH)\n",
        "    \n",
        "else:\n",
        "    sys.path.insert(0, LOCAL_PROJECT_PATH)\n",
        "\n",
        "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
        "print(\"‚úÖ Project setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ Install Dependencies\n",
        "if IN_COLAB:\n",
        "    print(\"üì¶ Installing dependencies for Colab...\")\n",
        "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    !pip install PyYAML scipy scikit-learn matplotlib seaborn tqdm tensorboard openpyxl\n",
        "else:\n",
        "    print(\"üì¶ Installing from requirements.txt...\")\n",
        "    if os.path.exists(\"requirements.txt\"):\n",
        "        !pip install -r requirements.txt\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Import Libraries and Setup\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yaml\n",
        "import json\n",
        "import subprocess\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime\n",
        "from IPython.display import clear_output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GPU setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üöÄ Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Training Monitor Class\n",
        "class TrainingMonitor:\n",
        "    def __init__(self):\n",
        "        self.metrics = {'train_loss': [], 'val_loss': [], 'epochs': []}\n",
        "        self.current_epoch = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.start_time = time.time()\n",
        "        \n",
        "    def parse_log_line(self, line):\n",
        "        try:\n",
        "            if 'Epoch' in line and '/' in line:\n",
        "                epoch_match = re.search(r'Epoch (\\d+)', line)\n",
        "                if epoch_match:\n",
        "                    self.current_epoch = int(epoch_match.group(1))\n",
        "                    \n",
        "            if 'Train Loss:' in line or 'Training Loss:' in line:\n",
        "                loss_match = re.search(r'Loss: ([\\d.]+)', line)\n",
        "                if loss_match:\n",
        "                    self.metrics['train_loss'].append(float(loss_match.group(1)))\n",
        "                    \n",
        "            if 'Val Loss:' in line or 'Validation Loss:' in line:\n",
        "                loss_match = re.search(r'Loss: ([\\d.]+)', line)\n",
        "                if loss_match:\n",
        "                    val_loss = float(loss_match.group(1))\n",
        "                    self.metrics['val_loss'].append(val_loss)\n",
        "                    if val_loss < self.best_val_loss:\n",
        "                        self.best_val_loss = val_loss\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    def plot_progress(self):\n",
        "        if not self.metrics['train_loss']:\n",
        "            return\n",
        "            \n",
        "        plt.figure(figsize=(12, 4))\n",
        "        \n",
        "        plt.subplot(1, 2, 1)\n",
        "        epochs = range(1, len(self.metrics['train_loss']) + 1)\n",
        "        plt.plot(epochs, self.metrics['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "        if self.metrics['val_loss']:\n",
        "            plt.plot(epochs, self.metrics['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "        plt.title(f'Training Progress - Epoch {self.current_epoch}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.subplot(1, 2, 2)\n",
        "        elapsed = time.time() - self.start_time\n",
        "        hours, remainder = divmod(elapsed, 3600)\n",
        "        minutes, seconds = divmod(remainder, 60)\n",
        "        \n",
        "        stats_text = f\"\"\"Training Statistics:\n",
        "        \n",
        "Current Epoch: {self.current_epoch}\n",
        "Best Val Loss: {self.best_val_loss:.4f}\n",
        "Training Time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\n",
        "        \"\"\"\n",
        "        plt.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
        "        plt.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "print(\"‚úÖ Training monitor ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ Training Function using existing train.py\n",
        "def run_training(model_type='original', epochs=50, batch_size=16, experiment_name=None):\n",
        "    \"\"\"Run training using the existing train.py script\"\"\"\n",
        "    \n",
        "    if experiment_name is None:\n",
        "        experiment_name = f\"{model_type}_cvae_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    \n",
        "    output_dir = f\"outputs_{experiment_name}\"\n",
        "    \n",
        "    # Create a simple config for the training\n",
        "    config = {\n",
        "        'data': {'sequence_length': 200, 'train_split': 0.7, 'val_split': 0.15, 'test_split': 0.15},\n",
        "        'model': {'type': model_type, 'static_dim': 4, 'latent_dim': 128, 'hidden_dim': 256},\n",
        "        'training': {'epochs': epochs, 'batch_size': batch_size, 'output_dir': output_dir}\n",
        "    }\n",
        "    \n",
        "    # Save config\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    config_path = f\"{output_dir}/config.yaml\"\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(config, f)\n",
        "    \n",
        "    # Prepare command\n",
        "    cmd = [\n",
        "        sys.executable, \"train.py\",\n",
        "        \"--config\", config_path,\n",
        "        \"--output-dir\", output_dir,\n",
        "        \"--device\", str(device),\n",
        "        \"--model\", model_type,\n",
        "        \"--epochs\", str(epochs),\n",
        "        \"--batch-size\", str(batch_size)\n",
        "    ]\n",
        "    \n",
        "    print(f\"üöÄ Starting {model_type} CVAE training...\")\n",
        "    print(f\"üìã Command: {' '.join(cmd)}\")\n",
        "    \n",
        "    # Initialize monitor\n",
        "    monitor = TrainingMonitor()\n",
        "    \n",
        "    try:\n",
        "        # Run training\n",
        "        process = subprocess.Popen(\n",
        "            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "            universal_newlines=True, bufsize=1\n",
        "        )\n",
        "        \n",
        "        line_count = 0\n",
        "        for line in iter(process.stdout.readline, ''):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                line_count += 1\n",
        "                \n",
        "                # Print important lines\n",
        "                if any(keyword in line for keyword in [\n",
        "                    'Epoch', 'Loss:', 'Starting', 'Best', 'Saved', 'completed', 'ERROR'\n",
        "                ]):\n",
        "                    print(line)\n",
        "                \n",
        "                # Parse metrics\n",
        "                monitor.parse_log_line(line)\n",
        "                \n",
        "                # Update plot every 50 lines\n",
        "                if line_count % 50 == 0 and monitor.metrics['train_loss']:\n",
        "                    clear_output(wait=True)\n",
        "                    print(f\"üîÑ Training in progress... (Line {line_count})\")\n",
        "                    monitor.plot_progress()\n",
        "        \n",
        "        # Final results\n",
        "        return_code = process.wait()\n",
        "        \n",
        "        if return_code == 0:\n",
        "            print(\"‚úÖ Training completed successfully!\")\n",
        "            \n",
        "            # Copy results to Drive if in Colab\n",
        "            if IN_COLAB and DRIVE_PROJECT_PATH:\n",
        "                drive_results_dir = f\"{DRIVE_PROJECT_PATH}/results/{experiment_name}\"\n",
        "                os.makedirs(f\"{DRIVE_PROJECT_PATH}/results\", exist_ok=True)\n",
        "                !cp -r \"{output_dir}\" \"{drive_results_dir}\"\n",
        "                print(f\"üíæ Results backed up to: {drive_results_dir}\")\n",
        "            \n",
        "            return output_dir\n",
        "        else:\n",
        "            print(f\"‚ùå Training failed with return code: {return_code}\")\n",
        "            return None\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Training error: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Training function ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ Evaluation Function using existing evaluate.py\n",
        "def run_evaluation(model_path=None, output_dir=None):\n",
        "    \"\"\"Run evaluation using the existing evaluate.py script\"\"\"\n",
        "    \n",
        "    # Find model if not specified\n",
        "    if model_path is None:\n",
        "        # Look for the most recent output directory\n",
        "        output_dirs = [d for d in os.listdir('.') if d.startswith('outputs_')]\n",
        "        if not output_dirs:\n",
        "            print(\"‚ùå No training outputs found. Please run training first.\")\n",
        "            return None\n",
        "        \n",
        "        latest_output_dir = max(output_dirs, key=lambda d: os.path.getmtime(d))\n",
        "        \n",
        "        # Look for best checkpoint\n",
        "        best_checkpoint = os.path.join(latest_output_dir, \"best_checkpoint.pth\")\n",
        "        if os.path.exists(best_checkpoint):\n",
        "            model_path = best_checkpoint\n",
        "        else:\n",
        "            # Look for any checkpoint\n",
        "            checkpoints = [f for f in os.listdir(latest_output_dir) if f.endswith('.pth')]\n",
        "            if checkpoints:\n",
        "                model_path = os.path.join(latest_output_dir, checkpoints[-1])\n",
        "            else:\n",
        "                print(f\"‚ùå No model checkpoints found in {latest_output_dir}\")\n",
        "                return None\n",
        "    \n",
        "    if output_dir is None:\n",
        "        output_dir = f\"evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    \n",
        "    # Prepare evaluation command\n",
        "    cmd = [\n",
        "        sys.executable, \"evaluate.py\",\n",
        "        \"--model\", model_path,\n",
        "        \"--output-dir\", output_dir,\n",
        "        \"--comprehensive\"\n",
        "    ]\n",
        "    \n",
        "    print(f\"üî¨ Starting evaluation...\")\n",
        "    print(f\"üìã Model: {model_path}\")\n",
        "    print(f\"üìã Command: {' '.join(cmd)}\")\n",
        "    \n",
        "    try:\n",
        "        # Run evaluation\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)  # 30 min timeout\n",
        "        \n",
        "        if result.stdout:\n",
        "            print(\"üìã Evaluation Output:\")\n",
        "            print(result.stdout)\n",
        "        \n",
        "        if result.stderr:\n",
        "            print(\"‚ö†Ô∏è Evaluation Warnings:\")\n",
        "            print(result.stderr)\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Evaluation completed successfully!\")\n",
        "            \n",
        "            # Copy results to Drive if in Colab\n",
        "            if IN_COLAB and DRIVE_PROJECT_PATH:\n",
        "                drive_eval_dir = f\"{DRIVE_PROJECT_PATH}/evaluations/{output_dir}\"\n",
        "                os.makedirs(f\"{DRIVE_PROJECT_PATH}/evaluations\", exist_ok=True)\n",
        "                !cp -r \"{output_dir}\" \"{drive_eval_dir}\"\n",
        "                print(f\"üíæ Evaluation results backed up to: {drive_eval_dir}\")\n",
        "            \n",
        "            return output_dir\n",
        "        else:\n",
        "            print(f\"‚ùå Evaluation failed with return code: {result.returncode}\")\n",
        "            return None\n",
        "            \n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"‚ùå Evaluation timed out after 30 minutes\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Evaluation error: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Evaluation function ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Quick Start Functions\n",
        "def quick_train_original(epochs=50):\n",
        "    \"\"\"Quick training with original CVAE model\"\"\"\n",
        "    return run_training(model_type='original', epochs=epochs, experiment_name='original_quick')\n",
        "\n",
        "def quick_train_advanced(epochs=100):\n",
        "    \"\"\"Quick training with advanced CVAE model\"\"\"\n",
        "    return run_training(model_type='advanced', epochs=epochs, experiment_name='advanced_quick')\n",
        "\n",
        "def quick_evaluate():\n",
        "    \"\"\"Quick evaluation of the most recent model\"\"\"\n",
        "    return run_evaluation()\n",
        "\n",
        "def compare_models():\n",
        "    \"\"\"Train and compare both models\"\"\"\n",
        "    print(\"üî¨ Starting model comparison...\")\n",
        "    \n",
        "    # Train original model\n",
        "    print(\"\\nüöÄ Training Original CVAE (30 epochs)...\")\n",
        "    original_output = run_training('original', epochs=30, experiment_name='comparison_original')\n",
        "    \n",
        "    if original_output:\n",
        "        print(\"\\nüî¨ Evaluating Original CVAE...\")\n",
        "        run_evaluation(f\"{original_output}/best_checkpoint.pth\", \"eval_original\")\n",
        "    \n",
        "    # Train advanced model  \n",
        "    print(\"\\nüöÄ Training Advanced CVAE (50 epochs)...\")\n",
        "    advanced_output = run_training('advanced', epochs=50, experiment_name='comparison_advanced')\n",
        "    \n",
        "    if advanced_output:\n",
        "        print(\"\\nüî¨ Evaluating Advanced CVAE...\")\n",
        "        run_evaluation(f\"{advanced_output}/best_checkpoint.pth\", \"eval_advanced\")\n",
        "    \n",
        "    print(\"\\nüìä Model comparison complete!\")\n",
        "    if IN_COLAB:\n",
        "        print(f\"üìÅ Check your Google Drive at: {DRIVE_PROJECT_PATH}/results/\")\n",
        "    \n",
        "    return original_output, advanced_output\n",
        "\n",
        "def list_results():\n",
        "    \"\"\"List all training and evaluation results\"\"\"\n",
        "    print(\"üìÅ Local Results:\")\n",
        "    \n",
        "    # Training outputs\n",
        "    outputs = [d for d in os.listdir('.') if d.startswith('outputs_')]\n",
        "    if outputs:\n",
        "        print(\"üöÄ Training Results:\")\n",
        "        for output in sorted(outputs):\n",
        "            print(f\"  - {output}\")\n",
        "    \n",
        "    # Evaluation results\n",
        "    evals = [d for d in os.listdir('.') if d.startswith('evaluation_')]\n",
        "    if evals:\n",
        "        print(\"üî¨ Evaluation Results:\")\n",
        "        for eval_dir in sorted(evals):\n",
        "            print(f\"  - {eval_dir}\")\n",
        "    \n",
        "    # Drive results (if in Colab)\n",
        "    if IN_COLAB and DRIVE_PROJECT_PATH:\n",
        "        print(f\"\\nüìÅ Google Drive Results: {DRIVE_PROJECT_PATH}/\")\n",
        "        if os.path.exists(f\"{DRIVE_PROJECT_PATH}/results\"):\n",
        "            drive_results = os.listdir(f\"{DRIVE_PROJECT_PATH}/results\")\n",
        "            if drive_results:\n",
        "                print(\"üöÄ Drive Training Results:\")\n",
        "                for result in sorted(drive_results):\n",
        "                    print(f\"  - {result}\")\n",
        "        \n",
        "        if os.path.exists(f\"{DRIVE_PROJECT_PATH}/evaluations\"):\n",
        "            drive_evals = os.listdir(f\"{DRIVE_PROJECT_PATH}/evaluations\")\n",
        "            if drive_evals:\n",
        "                print(\"üî¨ Drive Evaluation Results:\")\n",
        "                for eval_dir in sorted(drive_evals):\n",
        "                    print(f\"  - {eval_dir}\")\n",
        "\n",
        "print(\"‚úÖ Quick start functions ready!\")\n",
        "print(\"\\nüöÄ Available functions:\")\n",
        "print(\"  - quick_train_original(epochs=50)\")\n",
        "print(\"  - quick_train_advanced(epochs=100)\")\n",
        "print(\"  - quick_evaluate()\")\n",
        "print(\"  - compare_models()\")\n",
        "print(\"  - list_results()\")\n",
        "print(\"\\nüí° Example usage:\")\n",
        "print(\"  quick_train_original()  # Train original model\")\n",
        "print(\"  quick_evaluate()        # Evaluate latest model\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Ready to Use!\n",
        "\n",
        "**Your ABR CVAE project is now set up in Google Colab!**\n",
        "\n",
        "### üìã Quick Start:\n",
        "```python\n",
        "# Train original CVAE model (50 epochs)\n",
        "quick_train_original()\n",
        "\n",
        "# Train advanced CVAE model (100 epochs) \n",
        "quick_train_advanced()\n",
        "\n",
        "# Evaluate the most recent model\n",
        "quick_evaluate()\n",
        "\n",
        "# Compare both models\n",
        "compare_models()\n",
        "\n",
        "# List all results\n",
        "list_results()\n",
        "```\n",
        "\n",
        "### üìÅ File Structure:\n",
        "- **Local**: `/content/abr_project/` (working directory)\n",
        "- **Drive**: `/content/drive/MyDrive/abr_project/` (backup location)\n",
        "- **Results**: Automatically saved to both locations\n",
        "\n",
        "### üíæ Automatic Backup:\n",
        "All training outputs and evaluation results are automatically backed up to your Google Drive at:\n",
        "- Training: `/MyDrive/abr_project/results/`\n",
        "- Evaluations: `/MyDrive/abr_project/evaluations/`\n",
        "\n",
        "### üöÄ Start Training:\n",
        "Run the cell below to start training immediately!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ START TRAINING - Run this cell to begin!\n",
        "\n",
        "# Choose one of these options:\n",
        "\n",
        "# Option 1: Train original CVAE model (recommended for first run)\n",
        "quick_train_original(epochs=30)\n",
        "\n",
        "# Option 2: Train advanced CVAE model (more complex, takes longer)\n",
        "# quick_train_advanced(epochs=50)\n",
        "\n",
        "# Option 3: Compare both models (takes longest but most comprehensive)\n",
        "# compare_models()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ EVALUATE MODEL - Run this after training completes\n",
        "\n",
        "# Evaluate the most recent trained model\n",
        "quick_evaluate()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
