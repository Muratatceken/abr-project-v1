{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üß† ABR CVAE Training & Evaluation - Google Colab\n",
        "\n",
        "**Complete training and evaluation environment for ABR CVAE project in Google Colab**\n",
        "\n",
        "## üéØ Features:\n",
        "- **Uses Existing Project**: Works with your current train.py, evaluate.py, and src/ files\n",
        "- **Google Drive Storage**: Saves results to `/content/drive/MyDrive/abr_project/`\n",
        "- **Real-time Monitoring**: Live training progress visualization\n",
        "- **No New Files**: Uses only your existing project structure\n",
        "\n",
        "## üìã Setup Instructions:\n",
        "1. Upload your entire project to Google Drive at `/MyDrive/abr_project/`\n",
        "2. Run all cells in order\n",
        "3. Use the quick functions at the bottom to train and evaluate\n",
        "\n",
        "---\n",
        "*Murat At√ßeken - ABR CVAE Project*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Environment Setup\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"üîç Running in Google Colab\")\n",
        "    \n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Project paths\n",
        "    DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/abr_project\"\n",
        "    LOCAL_PROJECT_PATH = \"/content/abr_project\"\n",
        "    \n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"üîç Running locally\")\n",
        "    LOCAL_PROJECT_PATH = os.getcwd()\n",
        "    DRIVE_PROJECT_PATH = None\n",
        "\n",
        "print(f\"‚úÖ Environment detected: {'Colab' if IN_COLAB else 'Local'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìÅ Project Setup - Find project in Colab environment\n",
        "if IN_COLAB:\n",
        "    # Check multiple possible locations for the cloned project\n",
        "    possible_paths = [\n",
        "        \"/content/abr-project-v1\",  # Common clone name\n",
        "        \"/content/abr_project\",     # Alternative name\n",
        "        \"/content/abr-cvae-project\", # Another alternative\n",
        "        DRIVE_PROJECT_PATH,         # Google Drive location\n",
        "    ]\n",
        "    \n",
        "    # Also check for any directory containing train.py\n",
        "    content_dirs = [d for d in os.listdir('/content') if os.path.isdir(f'/content/{d}')]\n",
        "    for dir_name in content_dirs:\n",
        "        dir_path = f'/content/{dir_name}'\n",
        "        if os.path.exists(f'{dir_path}/train.py'):\n",
        "            possible_paths.insert(0, dir_path)  # Add to front of list\n",
        "    \n",
        "    project_found = False\n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(f\"{path}/train.py\"):\n",
        "            LOCAL_PROJECT_PATH = path\n",
        "            project_found = True\n",
        "            print(f\"‚úÖ Project found at: {LOCAL_PROJECT_PATH}\")\n",
        "            break\n",
        "    \n",
        "    if not project_found:\n",
        "        print(\"‚ùå Project not found in any expected location!\")\n",
        "        print(\"üîç Searched locations:\")\n",
        "        for path in possible_paths:\n",
        "            print(f\"  - {path}\")\n",
        "        print(\"\\nüí° Solutions:\")\n",
        "        print(\"1. Clone your repository: !git clone <your-repo-url>\")\n",
        "        print(\"2. Upload to Google Drive at: /MyDrive/abr_project\")\n",
        "        print(\"üìã Required files: train.py, evaluate.py, src/, configs/, data/\")\n",
        "        \n",
        "        # List current content directory to help debug\n",
        "        print(f\"\\nüìÅ Current /content directory contents:\")\n",
        "        for item in os.listdir('/content'):\n",
        "            item_path = f'/content/{item}'\n",
        "            if os.path.isdir(item_path):\n",
        "                print(f\"  üìÇ {item}/\")\n",
        "                # Check if it might be the project\n",
        "                if os.path.exists(f'{item_path}/train.py'):\n",
        "                    print(f\"    ‚úÖ Contains train.py - This might be your project!\")\n",
        "            else:\n",
        "                print(f\"  üìÑ {item}\")\n",
        "        \n",
        "        raise FileNotFoundError(\"Project files not found\")\n",
        "    \n",
        "    # Set working directory\n",
        "    os.chdir(LOCAL_PROJECT_PATH)\n",
        "    sys.path.insert(0, LOCAL_PROJECT_PATH)\n",
        "    \n",
        "else:\n",
        "    sys.path.insert(0, LOCAL_PROJECT_PATH)\n",
        "\n",
        "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
        "print(\"‚úÖ Project setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ Install Dependencies\n",
        "if IN_COLAB:\n",
        "    print(\"üì¶ Installing dependencies for Colab...\")\n",
        "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    !pip install PyYAML scipy scikit-learn matplotlib seaborn tqdm tensorboard openpyxl\n",
        "else:\n",
        "    print(\"üì¶ Installing from requirements.txt...\")\n",
        "    if os.path.exists(\"requirements.txt\"):\n",
        "        !pip install -r requirements.txt\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Import Libraries and Setup\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yaml\n",
        "import json\n",
        "import subprocess\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime\n",
        "from IPython.display import clear_output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GPU setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üöÄ Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Training Monitor Class\n",
        "class TrainingMonitor:\n",
        "    def __init__(self):\n",
        "        self.metrics = {'train_loss': [], 'val_loss': [], 'epochs': []}\n",
        "        self.current_epoch = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.start_time = time.time()\n",
        "        \n",
        "    def parse_log_line(self, line):\n",
        "        try:\n",
        "            if 'Epoch' in line and '/' in line:\n",
        "                epoch_match = re.search(r'Epoch (\\d+)', line)\n",
        "                if epoch_match:\n",
        "                    self.current_epoch = int(epoch_match.group(1))\n",
        "                    \n",
        "            if 'Train Loss:' in line or 'Training Loss:' in line:\n",
        "                loss_match = re.search(r'Loss: ([\\d.]+)', line)\n",
        "                if loss_match:\n",
        "                    self.metrics['train_loss'].append(float(loss_match.group(1)))\n",
        "                    \n",
        "            if 'Val Loss:' in line or 'Validation Loss:' in line:\n",
        "                loss_match = re.search(r'Loss: ([\\d.]+)', line)\n",
        "                if loss_match:\n",
        "                    val_loss = float(loss_match.group(1))\n",
        "                    self.metrics['val_loss'].append(val_loss)\n",
        "                    if val_loss < self.best_val_loss:\n",
        "                        self.best_val_loss = val_loss\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    def plot_progress(self):\n",
        "        if not self.metrics['train_loss']:\n",
        "            return\n",
        "            \n",
        "        plt.figure(figsize=(12, 4))\n",
        "        \n",
        "        plt.subplot(1, 2, 1)\n",
        "        epochs = range(1, len(self.metrics['train_loss']) + 1)\n",
        "        plt.plot(epochs, self.metrics['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "        if self.metrics['val_loss']:\n",
        "            plt.plot(epochs, self.metrics['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "        plt.title(f'Training Progress - Epoch {self.current_epoch}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.subplot(1, 2, 2)\n",
        "        elapsed = time.time() - self.start_time\n",
        "        hours, remainder = divmod(elapsed, 3600)\n",
        "        minutes, seconds = divmod(remainder, 60)\n",
        "        \n",
        "        stats_text = f\"\"\"Training Statistics:\n",
        "        \n",
        "Current Epoch: {self.current_epoch}\n",
        "Best Val Loss: {self.best_val_loss:.4f}\n",
        "Training Time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\n",
        "        \"\"\"\n",
        "        plt.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
        "        plt.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "print(\"‚úÖ Training monitor ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÑ Complete Data Preprocessing Function\n",
        "def run_preprocessing():\n",
        "    \"\"\"Complete data preprocessing from Excel file to processed pickle files\"\"\"\n",
        "    \n",
        "    print(\"üîÑ Starting complete data preprocessing pipeline...\")\n",
        "    \n",
        "    # Check if preprocessed data already exists\n",
        "    processed_data_paths = [\n",
        "        \"data/processed/abr_processed_data.pkl\",\n",
        "        \"data/processed/abr_data_preprocessed.pkl\",\n",
        "        \"data/abr_processed_data.pkl\"\n",
        "    ]\n",
        "    \n",
        "    for path in processed_data_paths:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"‚úÖ Preprocessed data found at: {path}\")\n",
        "            return True\n",
        "    \n",
        "    print(\"üìä No preprocessed data found. Starting from Excel file...\")\n",
        "    \n",
        "    # Look for Excel file in multiple locations\n",
        "    excel_paths = [\n",
        "        # Google Drive locations\n",
        "        f\"{DRIVE_PROJECT_PATH}/data/abr_data_preprocessed.xlsx\" if DRIVE_PROJECT_PATH else None,\n",
        "        f\"{DRIVE_PROJECT_PATH}/abr_data_preprocessed.xlsx\" if DRIVE_PROJECT_PATH else None,\n",
        "        \"/content/drive/MyDrive/abr_data_preprocessed.xlsx\",\n",
        "        \"/content/drive/MyDrive/data/abr_data_preprocessed.xlsx\",\n",
        "        # Local locations\n",
        "        \"data/abr_data_preprocessed.xlsx\",\n",
        "        \"data/raw/abr_data.xlsx\",\n",
        "        \"abr_data_preprocessed.xlsx\"\n",
        "    ]\n",
        "    \n",
        "    excel_file = None\n",
        "    for path in excel_paths:\n",
        "        if path and os.path.exists(path):\n",
        "            excel_file = path\n",
        "            print(f\"üìä Found Excel file at: {excel_file}\")\n",
        "            break\n",
        "    \n",
        "    if not excel_file:\n",
        "        print(\"‚ùå Excel file not found!\")\n",
        "        print(\"üîç Searched locations:\")\n",
        "        for path in excel_paths:\n",
        "            if path:\n",
        "                print(f\"  - {path}\")\n",
        "        print(\"\\nüí° Please upload your Excel file to one of these locations:\")\n",
        "        print(\"  - /content/drive/MyDrive/abr_data_preprocessed.xlsx\")\n",
        "        print(\"  - /content/drive/MyDrive/data/abr_data_preprocessed.xlsx\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        # Create necessary directories\n",
        "        os.makedirs(\"data/processed\", exist_ok=True)\n",
        "        os.makedirs(\"data/raw\", exist_ok=True)\n",
        "        \n",
        "        print(\"üìñ Loading Excel file...\")\n",
        "        # Load the Excel file\n",
        "        df = pd.read_excel(excel_file)\n",
        "        print(f\"‚úÖ Loaded Excel file with shape: {df.shape}\")\n",
        "        print(f\"üìã Columns: {list(df.columns)}\")\n",
        "        \n",
        "        # Copy Excel file to local data directory if not already there\n",
        "        local_excel_path = \"data/abr_data_preprocessed.xlsx\"\n",
        "        if not os.path.exists(local_excel_path):\n",
        "            import shutil\n",
        "            shutil.copy2(excel_file, local_excel_path)\n",
        "            print(f\"üìÅ Copied Excel file to: {local_excel_path}\")\n",
        "        \n",
        "        # Try to use existing preprocessing script first\n",
        "        preprocess_scripts = [\"preprocess.py\", \"process_data.py\", \"src/preprocess.py\"]\n",
        "        script_found = None\n",
        "        \n",
        "        for script in preprocess_scripts:\n",
        "            if os.path.exists(script):\n",
        "                script_found = script\n",
        "                break\n",
        "        \n",
        "        if script_found:\n",
        "            print(f\"üîÑ Using existing preprocessing script: {script_found}\")\n",
        "            cmd = [sys.executable, script_found]\n",
        "            \n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)\n",
        "            \n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ Preprocessing script completed successfully!\")\n",
        "                \n",
        "                # Check if output was created\n",
        "                for path in processed_data_paths:\n",
        "                    if os.path.exists(path):\n",
        "                        print(f\"‚úÖ Preprocessed data created at: {path}\")\n",
        "                        return True\n",
        "                        \n",
        "                print(\"‚ö†Ô∏è Script completed but no output found. Falling back to built-in preprocessing...\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Preprocessing script failed. Using built-in preprocessing...\")\n",
        "                if result.stderr:\n",
        "                    print(f\"Script error: {result.stderr}\")\n",
        "        \n",
        "        # Built-in preprocessing pipeline\n",
        "        print(\"üîÑ Running built-in preprocessing pipeline...\")\n",
        "        \n",
        "        # Basic preprocessing steps\n",
        "        print(\"üßπ Cleaning data...\")\n",
        "        \n",
        "        # Remove any completely empty rows/columns\n",
        "        df = df.dropna(how='all').dropna(axis=1, how='all')\n",
        "        \n",
        "        # Basic data validation\n",
        "        print(f\"üìä Data shape after cleaning: {df.shape}\")\n",
        "        \n",
        "        # Look for waveform data columns (typically numeric columns)\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        print(f\"üìà Found {len(numeric_cols)} numeric columns (potential waveform data)\")\n",
        "        \n",
        "        # Look for clinical data columns (typically contain keywords)\n",
        "        clinical_keywords = ['latency', 'amplitude', 'threshold', 'wave', 'peak']\n",
        "        clinical_cols = []\n",
        "        for col in df.columns:\n",
        "            if any(keyword.lower() in col.lower() for keyword in clinical_keywords):\n",
        "                clinical_cols.append(col)\n",
        "        \n",
        "        print(f\"üè• Found {len(clinical_cols)} potential clinical columns: {clinical_cols}\")\n",
        "        \n",
        "        # Create a basic processed dataset structure\n",
        "        processed_data = {\n",
        "            'raw_data': df,\n",
        "            'waveform_columns': numeric_cols,\n",
        "            'clinical_columns': clinical_cols,\n",
        "            'metadata': {\n",
        "                'original_shape': df.shape,\n",
        "                'processing_date': datetime.now().isoformat(),\n",
        "                'source_file': excel_file\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Save processed data\n",
        "        output_path = \"data/processed/abr_processed_data.pkl\"\n",
        "        \n",
        "        print(f\"üíæ Saving processed data to: {output_path}\")\n",
        "        import pickle\n",
        "        with open(output_path, 'wb') as f:\n",
        "            pickle.dump(processed_data, f)\n",
        "        \n",
        "        print(\"‚úÖ Built-in preprocessing completed successfully!\")\n",
        "        print(f\"üìä Processed data saved with {len(df)} samples\")\n",
        "        print(f\"üìà Waveform columns: {len(numeric_cols)}\")\n",
        "        print(f\"üè• Clinical columns: {len(clinical_cols)}\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Preprocessing failed with error: {e}\")\n",
        "        import traceback\n",
        "        print(\"üîç Full error traceback:\")\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "def check_data_status():\n",
        "    \"\"\"Check the current status of data files\"\"\"\n",
        "    print(\"üìä Data Status Check:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Check for Excel files\n",
        "    excel_paths = [\n",
        "        f\"{DRIVE_PROJECT_PATH}/data/abr_data_preprocessed.xlsx\" if DRIVE_PROJECT_PATH else None,\n",
        "        f\"{DRIVE_PROJECT_PATH}/abr_data_preprocessed.xlsx\" if DRIVE_PROJECT_PATH else None,\n",
        "        \"/content/drive/MyDrive/abr_data_preprocessed.xlsx\",\n",
        "        \"data/abr_data_preprocessed.xlsx\",\n",
        "        \"abr_data_preprocessed.xlsx\"\n",
        "    ]\n",
        "    \n",
        "    print(\"üìÅ Excel Files:\")\n",
        "    excel_found = False\n",
        "    for path in excel_paths:\n",
        "        if path and os.path.exists(path):\n",
        "            size = os.path.getsize(path) / (1024*1024)  # MB\n",
        "            print(f\"  ‚úÖ {path} ({size:.1f} MB)\")\n",
        "            excel_found = True\n",
        "        elif path:\n",
        "            print(f\"  ‚ùå {path}\")\n",
        "    \n",
        "    if not excel_found:\n",
        "        print(\"  ‚ö†Ô∏è No Excel files found!\")\n",
        "    \n",
        "    # Check for processed files\n",
        "    processed_paths = [\n",
        "        \"data/processed/abr_processed_data.pkl\",\n",
        "        \"data/processed/abr_data_preprocessed.pkl\",\n",
        "        \"data/abr_processed_data.pkl\"\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nüîÑ Processed Files:\")\n",
        "    processed_found = False\n",
        "    for path in processed_paths:\n",
        "        if os.path.exists(path):\n",
        "            size = os.path.getsize(path) / (1024*1024)  # MB\n",
        "            print(f\"  ‚úÖ {path} ({size:.1f} MB)\")\n",
        "            processed_found = True\n",
        "        else:\n",
        "            print(f\"  ‚ùå {path}\")\n",
        "    \n",
        "    if not processed_found:\n",
        "        print(\"  ‚ö†Ô∏è No processed files found!\")\n",
        "    \n",
        "    # Check preprocessing scripts\n",
        "    scripts = [\"preprocess.py\", \"process_data.py\", \"src/preprocess.py\"]\n",
        "    print(\"\\nüîß Preprocessing Scripts:\")\n",
        "    for script in scripts:\n",
        "        if os.path.exists(script):\n",
        "            print(f\"  ‚úÖ {script}\")\n",
        "        else:\n",
        "            print(f\"  ‚ùå {script}\")\n",
        "    \n",
        "    return excel_found, processed_found\n",
        "\n",
        "print(\"‚úÖ Complete preprocessing pipeline ready!\")\n",
        "print(\"üìã Available functions:\")\n",
        "print(\"  - run_preprocessing(): Complete preprocessing from Excel to pickle\")\n",
        "print(\"  - check_data_status(): Check current data file status\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ Training Function using existing train.py\n",
        "def run_training(model_type='original', epochs=50, batch_size=16, experiment_name=None):\n",
        "    \"\"\"Run training using the existing train.py script\"\"\"\n",
        "    \n",
        "    # Check if preprocessing is needed\n",
        "    processed_data_paths = [\n",
        "        \"data/processed/abr_processed_data.pkl\",\n",
        "        \"data/processed/abr_data_preprocessed.pkl\", \n",
        "        \"data/abr_processed_data.pkl\"\n",
        "    ]\n",
        "    \n",
        "    data_exists = any(os.path.exists(path) for path in processed_data_paths)\n",
        "    \n",
        "    if not data_exists:\n",
        "        print(\"üîÑ Preprocessed data not found. Running preprocessing first...\")\n",
        "        if not run_preprocessing():\n",
        "            print(\"‚ùå Preprocessing failed. Cannot proceed with training.\")\n",
        "            return None\n",
        "        print(\"‚úÖ Preprocessing completed. Starting training...\")\n",
        "    \n",
        "    if experiment_name is None:\n",
        "        experiment_name = f\"{model_type}_cvae_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    \n",
        "    output_dir = f\"outputs_{experiment_name}\"\n",
        "    \n",
        "    # Create a simple config for the training\n",
        "    config = {\n",
        "        'data': {'sequence_length': 200, 'train_split': 0.7, 'val_split': 0.15, 'test_split': 0.15},\n",
        "        'model': {'type': model_type, 'static_dim': 4, 'latent_dim': 128, 'hidden_dim': 256},\n",
        "        'training': {'epochs': epochs, 'batch_size': batch_size, 'output_dir': output_dir}\n",
        "    }\n",
        "    \n",
        "    # Save config\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    config_path = f\"{output_dir}/config.yaml\"\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(config, f)\n",
        "    \n",
        "    # Prepare command\n",
        "    cmd = [\n",
        "        sys.executable, \"train.py\",\n",
        "        \"--config\", config_path,\n",
        "        \"--output-dir\", output_dir,\n",
        "        \"--device\", str(device),\n",
        "        \"--model\", model_type,\n",
        "        \"--epochs\", str(epochs),\n",
        "        \"--batch-size\", str(batch_size)\n",
        "    ]\n",
        "    \n",
        "    print(f\"üöÄ Starting {model_type} CVAE training...\")\n",
        "    print(f\"üìã Command: {' '.join(cmd)}\")\n",
        "    \n",
        "    # Initialize monitor\n",
        "    monitor = TrainingMonitor()\n",
        "    \n",
        "    try:\n",
        "        # Run training\n",
        "        process = subprocess.Popen(\n",
        "            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "            universal_newlines=True, bufsize=1\n",
        "        )\n",
        "        \n",
        "        line_count = 0\n",
        "        for line in iter(process.stdout.readline, ''):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                line_count += 1\n",
        "                \n",
        "                # Print important lines\n",
        "                if any(keyword in line for keyword in [\n",
        "                    'Epoch', 'Loss:', 'Starting', 'Best', 'Saved', 'completed', 'ERROR'\n",
        "                ]):\n",
        "                    print(line)\n",
        "                \n",
        "                # Parse metrics\n",
        "                monitor.parse_log_line(line)\n",
        "                \n",
        "                # Update plot every 50 lines\n",
        "                if line_count % 50 == 0 and monitor.metrics['train_loss']:\n",
        "                    clear_output(wait=True)\n",
        "                    print(f\"üîÑ Training in progress... (Line {line_count})\")\n",
        "                    monitor.plot_progress()\n",
        "        \n",
        "        # Final results\n",
        "        return_code = process.wait()\n",
        "        \n",
        "        if return_code == 0:\n",
        "            print(\"‚úÖ Training completed successfully!\")\n",
        "            \n",
        "            # Copy results to Drive if in Colab\n",
        "            if IN_COLAB and DRIVE_PROJECT_PATH:\n",
        "                drive_results_dir = f\"{DRIVE_PROJECT_PATH}/results/{experiment_name}\"\n",
        "                os.makedirs(f\"{DRIVE_PROJECT_PATH}/results\", exist_ok=True)\n",
        "                !cp -r \"{output_dir}\" \"{drive_results_dir}\"\n",
        "                print(f\"üíæ Results backed up to: {drive_results_dir}\")\n",
        "            \n",
        "            return output_dir\n",
        "        else:\n",
        "            print(f\"‚ùå Training failed with return code: {return_code}\")\n",
        "            return None\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Training error: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Training function ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ Evaluation Function using existing evaluate.py\n",
        "def run_evaluation(model_path=None, output_dir=None):\n",
        "    \"\"\"Run evaluation using the existing evaluate.py script\"\"\"\n",
        "    \n",
        "    # Find model if not specified\n",
        "    if model_path is None:\n",
        "        # Look for the most recent output directory\n",
        "        output_dirs = [d for d in os.listdir('.') if d.startswith('outputs_')]\n",
        "        if not output_dirs:\n",
        "            print(\"‚ùå No training outputs found. Please run training first.\")\n",
        "            return None\n",
        "        \n",
        "        latest_output_dir = max(output_dirs, key=lambda d: os.path.getmtime(d))\n",
        "        \n",
        "        # Look for best checkpoint\n",
        "        best_checkpoint = os.path.join(latest_output_dir, \"best_checkpoint.pth\")\n",
        "        if os.path.exists(best_checkpoint):\n",
        "            model_path = best_checkpoint\n",
        "        else:\n",
        "            # Look for any checkpoint\n",
        "            checkpoints = [f for f in os.listdir(latest_output_dir) if f.endswith('.pth')]\n",
        "            if checkpoints:\n",
        "                model_path = os.path.join(latest_output_dir, checkpoints[-1])\n",
        "            else:\n",
        "                print(f\"‚ùå No model checkpoints found in {latest_output_dir}\")\n",
        "                return None\n",
        "    \n",
        "    if output_dir is None:\n",
        "        output_dir = f\"evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    \n",
        "    # Prepare evaluation command\n",
        "    cmd = [\n",
        "        sys.executable, \"evaluate.py\",\n",
        "        \"--model\", model_path,\n",
        "        \"--output-dir\", output_dir,\n",
        "        \"--comprehensive\"\n",
        "    ]\n",
        "    \n",
        "    print(f\"üî¨ Starting evaluation...\")\n",
        "    print(f\"üìã Model: {model_path}\")\n",
        "    print(f\"üìã Command: {' '.join(cmd)}\")\n",
        "    \n",
        "    try:\n",
        "        # Run evaluation\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)  # 30 min timeout\n",
        "        \n",
        "        if result.stdout:\n",
        "            print(\"üìã Evaluation Output:\")\n",
        "            print(result.stdout)\n",
        "        \n",
        "        if result.stderr:\n",
        "            print(\"‚ö†Ô∏è Evaluation Warnings:\")\n",
        "            print(result.stderr)\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Evaluation completed successfully!\")\n",
        "            \n",
        "            # Copy results to Drive if in Colab\n",
        "            if IN_COLAB and DRIVE_PROJECT_PATH:\n",
        "                drive_eval_dir = f\"{DRIVE_PROJECT_PATH}/evaluations/{output_dir}\"\n",
        "                os.makedirs(f\"{DRIVE_PROJECT_PATH}/evaluations\", exist_ok=True)\n",
        "                !cp -r \"{output_dir}\" \"{drive_eval_dir}\"\n",
        "                print(f\"üíæ Evaluation results backed up to: {drive_eval_dir}\")\n",
        "            \n",
        "            return output_dir\n",
        "        else:\n",
        "            print(f\"‚ùå Evaluation failed with return code: {result.returncode}\")\n",
        "            return None\n",
        "            \n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"‚ùå Evaluation timed out after 30 minutes\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Evaluation error: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Evaluation function ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Quick Start Functions\n",
        "def quick_train_original(epochs=50):\n",
        "    \"\"\"Quick training with original CVAE model\"\"\"\n",
        "    return run_training(model_type='original', epochs=epochs, experiment_name='original_quick')\n",
        "\n",
        "def quick_train_advanced(epochs=100):\n",
        "    \"\"\"Quick training with advanced CVAE model\"\"\"\n",
        "    return run_training(model_type='advanced', epochs=epochs, experiment_name='advanced_quick')\n",
        "\n",
        "def quick_evaluate():\n",
        "    \"\"\"Quick evaluation of the most recent model\"\"\"\n",
        "    return run_evaluation()\n",
        "\n",
        "def compare_models():\n",
        "    \"\"\"Train and compare both models\"\"\"\n",
        "    print(\"üî¨ Starting model comparison...\")\n",
        "    \n",
        "    # Train original model\n",
        "    print(\"\\nüöÄ Training Original CVAE (30 epochs)...\")\n",
        "    original_output = run_training('original', epochs=30, experiment_name='comparison_original')\n",
        "    \n",
        "    if original_output:\n",
        "        print(\"\\nüî¨ Evaluating Original CVAE...\")\n",
        "        run_evaluation(f\"{original_output}/best_checkpoint.pth\", \"eval_original\")\n",
        "    \n",
        "    # Train advanced model  \n",
        "    print(\"\\nüöÄ Training Advanced CVAE (50 epochs)...\")\n",
        "    advanced_output = run_training('advanced', epochs=50, experiment_name='comparison_advanced')\n",
        "    \n",
        "    if advanced_output:\n",
        "        print(\"\\nüî¨ Evaluating Advanced CVAE...\")\n",
        "        run_evaluation(f\"{advanced_output}/best_checkpoint.pth\", \"eval_advanced\")\n",
        "    \n",
        "    print(\"\\nüìä Model comparison complete!\")\n",
        "    if IN_COLAB:\n",
        "        print(f\"üìÅ Check your Google Drive at: {DRIVE_PROJECT_PATH}/results/\")\n",
        "    \n",
        "    return original_output, advanced_output\n",
        "\n",
        "def list_results():\n",
        "    \"\"\"List all training and evaluation results\"\"\"\n",
        "    print(\"üìÅ Local Results:\")\n",
        "    \n",
        "    # Training outputs\n",
        "    outputs = [d for d in os.listdir('.') if d.startswith('outputs_')]\n",
        "    if outputs:\n",
        "        print(\"üöÄ Training Results:\")\n",
        "        for output in sorted(outputs):\n",
        "            print(f\"  - {output}\")\n",
        "    \n",
        "    # Evaluation results\n",
        "    evals = [d for d in os.listdir('.') if d.startswith('evaluation_')]\n",
        "    if evals:\n",
        "        print(\"üî¨ Evaluation Results:\")\n",
        "        for eval_dir in sorted(evals):\n",
        "            print(f\"  - {eval_dir}\")\n",
        "    \n",
        "    # Drive results (if in Colab)\n",
        "    if IN_COLAB and DRIVE_PROJECT_PATH:\n",
        "        print(f\"\\nüìÅ Google Drive Results: {DRIVE_PROJECT_PATH}/\")\n",
        "        if os.path.exists(f\"{DRIVE_PROJECT_PATH}/results\"):\n",
        "            drive_results = os.listdir(f\"{DRIVE_PROJECT_PATH}/results\")\n",
        "            if drive_results:\n",
        "                print(\"üöÄ Drive Training Results:\")\n",
        "                for result in sorted(drive_results):\n",
        "                    print(f\"  - {result}\")\n",
        "        \n",
        "        if os.path.exists(f\"{DRIVE_PROJECT_PATH}/evaluations\"):\n",
        "            drive_evals = os.listdir(f\"{DRIVE_PROJECT_PATH}/evaluations\")\n",
        "            if drive_evals:\n",
        "                print(\"üî¨ Drive Evaluation Results:\")\n",
        "                for eval_dir in sorted(drive_evals):\n",
        "                    print(f\"  - {eval_dir}\")\n",
        "\n",
        "print(\"‚úÖ Quick start functions ready!\")\n",
        "print(\"\\nüöÄ Available functions:\")\n",
        "print(\"  - quick_train_original(epochs=50)\")\n",
        "print(\"  - quick_train_advanced(epochs=100)\")\n",
        "print(\"  - quick_evaluate()\")\n",
        "print(\"  - compare_models()\")\n",
        "print(\"  - list_results()\")\n",
        "print(\"\\nüí° Example usage:\")\n",
        "print(\"  quick_train_original()  # Train original model\")\n",
        "print(\"  quick_evaluate()        # Evaluate latest model\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Ready to Use!\n",
        "\n",
        "**Your ABR CVAE project is now set up in Google Colab!**\n",
        "\n",
        "### üìã Quick Start:\n",
        "```python\n",
        "# Train original CVAE model (50 epochs)\n",
        "quick_train_original()\n",
        "\n",
        "# Train advanced CVAE model (100 epochs) \n",
        "quick_train_advanced()\n",
        "\n",
        "# Evaluate the most recent model\n",
        "quick_evaluate()\n",
        "\n",
        "# Compare both models\n",
        "compare_models()\n",
        "\n",
        "# List all results\n",
        "list_results()\n",
        "```\n",
        "\n",
        "### üìÅ File Structure:\n",
        "- **Local**: `/content/abr_project/` (working directory)\n",
        "- **Drive**: `/content/drive/MyDrive/abr_project/` (backup location)\n",
        "- **Results**: Automatically saved to both locations\n",
        "\n",
        "### üíæ Automatic Backup:\n",
        "All training outputs and evaluation results are automatically backed up to your Google Drive at:\n",
        "- Training: `/MyDrive/abr_project/results/`\n",
        "- Evaluations: `/MyDrive/abr_project/evaluations/`\n",
        "\n",
        "### üöÄ Start Training:\n",
        "Run the cell below to start training immediately!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ START TRAINING - Run this cell to begin!\n",
        "\n",
        "# Choose one of these options:\n",
        "\n",
        "# Option 1: Train original CVAE model (recommended for first run)\n",
        "quick_train_original(epochs=30)\n",
        "\n",
        "# Option 2: Train advanced CVAE model (more complex, takes longer)\n",
        "# quick_train_advanced(epochs=50)\n",
        "\n",
        "# Option 3: Compare both models (takes longest but most comprehensive)\n",
        "# compare_models()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÑ DATA STATUS & PREPROCESSING\n",
        "\n",
        "# First, check what data files are available\n",
        "print(\"üîç Checking current data status...\")\n",
        "check_data_status()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üîÑ Starting preprocessing...\")\n",
        "\n",
        "# Run complete preprocessing from Excel file\n",
        "run_preprocessing()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÑ RUN PREPROCESSING - Run this if you get data preprocessing errors\n",
        "\n",
        "# This will automatically run when needed, but you can also run it manually\n",
        "run_preprocessing()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ EVALUATE MODEL - Run this after training completes\n",
        "\n",
        "# Evaluate the most recent trained model\n",
        "quick_evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ EVALUATE MODEL - Run this after training completes\n",
        "\n",
        "# Evaluate the most recent trained model\n",
        "quick_evaluate()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
