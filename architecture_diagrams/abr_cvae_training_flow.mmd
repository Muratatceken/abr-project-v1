graph TD
    %% Training Pipeline
    A["ABR Dataset<br/>ğŸ“Š 22,746 samples<br/>ğŸ“ˆ Train: 15,922<br/>ğŸ“‰ Val: 3,411<br/>ğŸ§ª Test: 3,413"] --> B["Data Preprocessing"]
    
    B --> C["Batch Creation<br/>Batch Size: 32<br/>Sequence Length: 200"]
    
    %% Training Loop
    C --> D["Forward Pass"]
    D --> E["Loss Computation"]
    E --> F["Backward Pass"]
    F --> G["Optimizer Step<br/>Adam LR: 0.0001"]
    G --> H["Validation"]
    
    %% Loss Components
    E --> I["ABR Reconstruction Loss<br/>MSE(original, reconstructed)"]
    E --> J["Masked Features Loss<br/>Latency + Amplitude"]
    E --> K["KL Divergence Loss<br/>Î²-annealing: 0.0 â†’ 0.005"]
    
    I --> L["Total Loss<br/>Weighted Combination"]
    J --> L
    K --> L
    
    %% Training Progress
    H --> M["Epoch Progress<br/>ğŸ¯ Current: 6/80<br/>â±ï¸ Time: ~1.7 min/epoch<br/>ğŸ“ˆ Val Loss: 76.14 â†’ 63.80<br/>âœ… Consistent Improvement"]
    
    M --> N{"Early Stopping?<br/>Patience: 15 epochs"}
    N -->|No| D
    N -->|Yes| O["Training Complete"]
    
    %% Checkpointing
    H --> P["Best Model Saving<br/>ğŸ’¾ Auto-checkpoint<br/>ğŸ“Š TensorBoard Logging<br/>ğŸ“ˆ Training History"]
    
    %% Current Status
    Q["Current Training Status<br/>ğŸš€ Model: 520k parameters<br/>âš¡ Speed: 5.2 it/s<br/>ğŸ“‰ Stable convergence<br/>ğŸ¯ Beta: 0.000033<br/>ğŸ“Š KL Loss: 39.44"]
    
    %% Styling
    classDef data fill:#e8f5e8
    classDef training fill:#fff3e0
    classDef loss fill:#ffebee
    classDef progress fill:#e1f5fe
    classDef status fill:#f3e5f5

    class A,B,C data
    class D,F,G,H training
    class I,J,K,L loss
    class M,N,O,P progress
    class Q status